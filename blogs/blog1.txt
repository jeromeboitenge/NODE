    1. Introduction 
    1. Describe the main purpose of an operating system.

The main purpose of an OS is to act as an intermediary between the user of a computer and the computer hardware by providing an environment in which a user can execute programs in a convenient and efficient manner.

    2. Identify 3 activities of an operating system in respect to process management, memory management, I/O management and file management. 
	Process Management:
        ◦ Process creation and deletion.
        ◦ Process suspension and resumption.
        ◦ Provision of mechanisms for:
            i. process synchronization
            ii. process communication
	Memory Management:
                • Keep track of which parts of memory are currently being used and by whom.
                • Decide which processes to load when memory space becomes available.
                • Allocate and deallocate memory space as needed.
	File Management:
    • File creation and deletion.
    • Directory creation and deletion.
    • Support of primitives for manipulating files and directories.
    • Mapping files onto secondary storage.
    • File backup on stable (nonvolatile) storage media.
	I/O Management:
        ◦ A buffer-caching system 
        ◦ A general device-driver interface
        ◦ Drivers for specific hardware devices
	Secondary Storage Management:
    • Free space management
    • Storage allocation
    • Disk scheduling
	
    3. Name two storage media with random access and two media with sequential access.
	Random Access:
    • RAM
    • Registers
    • Cache
	Sequential Access:
    • Magnetic Disks???
    • Optical Disks
    • Magnetic Tapes

    4. What are the differences between a trap and an interrupt? What is the use of each function? 

	A trap is a software-generated interrupt caused either by an error or a specific 	user program request.
An Interrupt is an asynchronous break in program flow that occurs as a result of events outside the running program. It’s usually hardware related, stemming from events such as a button press, timer expiration, or completion of a data transfer. An Interrupt triggers execution of instructions that perform work on behalf of the system, but not necessarily the current program. They were developed for exception handling and were later applied to I/O events.
    5. What are the purposes of system calls and system programs?

System calls provide an interface between the process and the operating system. System calls allow user-level processes to request some services from the operating system which process itself is not allowed to do. For example, for I/O a process involves a system call telling the operating system to read or write particular area and this request is satisfied by the operating system.
	
The purpose of a system program is to provide a convenient environment for   program development and execution.  


    6. What are the main advantages of multiprogramming?

    • Increases CPU utilization by organizing  jobs so that the CPU always has one to execute
    • Allows time sharing: many users can use a computer system interactively at the same time

	
    7. What is multi tasking, multi programming, multi threading? 

Multi tasking: The ability to execute more than one task at the same time

Multi programming: The ability to run several programs on a uniprocessor.

Multi threading:The ability of an operating system to execute different parts of a program, called threads, simultaneously. ***??

    8. What is a Real-Time System? 

A real-time system is one with well defined, fixed time constraints whereby processing must be done within the defined constraints or the system will fail


    9. What is the difference between Hard and Soft real-time systems? What is a mission critical system?
    • Hard real-time
        ◦ Secondary storage limited or absent, data stored in short term memory, or read-only memory (ROM)
        ◦ Conflicts with time-sharing systems, not supported by general-purpose operating systems.
        ◦ Guarantees that all critical tasks be completed on time
    • Soft real-time
        ◦ Limited utility in industrial control of robotics
        ◦ Useful in applications (multimedia, virtual reality) requiring advanced operating-system features.
        ◦ A critical real-time task gets priority over other tasks and retains that priority until it completes

    • Mission Critical System: 
	


    2. Processes

    10. Explain the difference between a program and a process.

A process is a program loaded into memory and executing. It is a unit of work in a system. 

A program is lines of code or algorithm expressed in some suitable notation.

    11. What are typical process states and typical transitions between these states.

	As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • Waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.



 


Transitions between the states

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → Waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out  (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running. 
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    12. Why are program counter and processor registers part of the process control block?
The process control block represents each process in the O/S. 

    • The program counter contains the address of the next instruction for execution for 	this process
    • The processor registers are a volatile storage medium that are used to store the current state of a process when an interrupt occurs. They allow the process to be 	continued correctly afterwards 

    13. Explain the main differences between processes and threads.
Differences
    • Unlike processes, threads are not independent of one another. 
    • Unlike processes, all threads can access every address in the task. 
    • Unlike processes, threads are designed to assist one other. Note that processes might or might not assist one another because processes may originate from different users. 
	Similarities
    • Like processes threads share CPU and only one thread active (running) at a time. 
    • Like processes, threads within a process, execute sequentially. 
    • Like processes, threads can create children. 
    • And like process, if one thread is blocked, another thread can run. 

    14. What are the requirements for a good scheduler?

    • Max CPU utilization – keep the CPU as busy as possible
    • Max Throughput – Number of processes that complete their execution per time unit
    • Min Turnaround time – amount of time to execute a particular process
    • Min Waiting time – amount of time a process has been waiting in the ready queue
    • Min Response time – amount of time it takes from when a request was submitted until the first response is produced, not output  (for time-sharing environment)

    15. Explain the difference between preemptive and non-preemptive schedulers.

    • Nonpreemptive – once CPU given to the process it cannot be interrupted until it completes its CPU burst or switches to a waiting state.
    • Preemptive – if a new process arrives with CPU burst length less than remaining time of current executing process, preempt.  This scheme is known as the Shortest-Remaining-Time-First (SRTF).

Explain why strict nonpreemptive scheduling is unlikely to be used in a computer center.
	Convoy effect, short jobs are made to wait by longer jobs

    16. Explain the scheduling strategies FCFS, LPTF, SJF, SRTF, and RR.

    • FCFS: (First Come First Served) process that requests the CPU first, gets the CPU first (FIFO)

    • SJF: (Shortest Job First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the smallest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    • SRTF: (Shortest Remaining Time First) pre-emptive SJF scheduling: When a process with a shorter next CPU burst than what is left of the currently executing process arrives in the ready queue, it will preempt the currently executing process

    • RR: (Round Robin scheduling) designed especially for timesharing systems. Similar to FCFS but preemption is added to switch between processes. A small unit called a time quantum or time slice is defined. The ready queue is treated as a circular queue. The CPU scheduler goes around the ready queue allocating the CPU for each process for a time interval of up to one time quantum


    • LPTF: (Long Processing Time First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the longest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    17. What are two differences between user-level threads and kernel-level threads? Under what circumstances is one type better than the another?

    • User-level threads are faster to create and require less overhead than kernel-level threads.
    • The kernel knows about kernel-level threads, but not user-level threads, so K/L threads can be scheduled by the kernel’s scheduler.

K/L threads are better than U/L threads when you will have threads that block, for example, on I/O. In these cases, the scheduler can choose another K/L to run so that the whole process doesn’t block.

U/L threads are better when there will be a lot of them created, or when they will be created and destroyed relatively quickly because there is little creation/destruction overhead. They are also better when the threads need to communicate because thread communication is simply a procedure call in U/L threads, which is faster than going through the kernel.

Short Answer
    • User threads are built using a library and are unknown or undifferentiated by the kernel. 
    • Kernel threads require more operating system resources because they are managed by the kernel. 
A user thread is more appropriate for low-level tasks, whereas a kernel thread is better for high-priority tasks that should get high priority to system resources

    18. What resources are used when a thread is created? How do they differ from those used when a process is created?

When a thread is created, it needs its own execution state. This basically involves allocating it memory for its stack, and giving it a PC, SP, and storage for its other registers, all within the processes address space.

When a process is created, it receives an entirely new address space, which means that the system has to create a new mapping from addresses to physical memory, and it has to allocate the process a new block of physical memory. Within that address space, the system must keep track of the open files, the process’s permissions, etc. It must also allocate things that are required for each thread within that process to have its own execution state, including a place in memory to store the stack and the registers for each thread in the process, as listed above.

So the resources used in creating a thread are a strict (and much smaller) subset of those used when creating a process.

Short Answer
When created, a thread requires an ID, a register set, a stack and program counter. 
Threads are smaller than processes. In addition to the thread information, each process creates its own Process Control Block (PCB), containing a code section, data section, and O/S resources. Threads reference these items from their creating or parent process.

    19. Consider the following set of processes, with the length of the CPU-burst time given in milliseconds: 
Process
Burst Time 
Priority
P1
10
3
P2
1
1
P3
2
3
P4
1
4
P5
5
2
The processes are assumed to have arrived in the order PI, P2, P3, P4, P5, all at time 0 
        a. Draw four Gantt charts that illustrate the execution of these processes using FCFS, SJF, a nonpreemptive priority (a smaller priority number implies a higher priority), and RR (quantum = 1) scheduling.
FCFS
P1
P2
P3
P4
P5
	          0			           10     11              13     14                             19
		      SJF	
P2
P4
P3
P5
P1
		           0      1	     2                4                               9                                              19


No preemptive Priority
P2
P5
P1
P3
P4
     0      1                             6                                       16                    18    19
Round Robin
P1
P2
P3
P4
P5
P1
P3
P5
P1
P5
P1
P5
P1
P5
P1
P1
P1
P1
P1
0     1      2     3      4      5      6      7      8      9     10   11   12     13   14     15    16   17    18      19

        b. What is the turnaround time of each process for each of the scheduling algorithms in part a?
Turnaround time = Completion time – Arrival time 
FCFS
P1, 10 – 0 = 10, P2, 11 – 0 = 11, P3, 13 – 0 = 13, P4, 14 – 0 = 14, P5, 19 – 0 = 19
SJF 
P1, 19 – 0 = 19, P2, 1 – 0 = 1, P3, 4 – 0 = 4, P4, 2 – 0 = 2, P5, 9 – 0 = 9
No preemptive Priority
P1, 16 – 0 = 16, P2, 1 – 0 = 1, P3, 18 – 0 = 18, P4, 19 – 0 = 19, P5, 6 – 0 = 6
Round Robin
P1, 19 – 0 = 19, P2, 2 – 0 = 2, P3, 7 – 0 = 7, P4, 4 – 0 = 4, P5, 14 – 0 = 14
        c. What is the waiting time of each process for each of the scheduling algorithms in part a?
Waiting time = Turnaround time – Burst time
FCFS
P1, 10 – 10 = 0, P2, 11 – 1 = 10, P3, 13 – 2 = 11, P4, 14 – 1 = 13, P5, 19 – 5 = 14
SJF 
P1, 19 – 10 = 9, P2, 1 – 1 = 0, P3, 4 – 2 = 2, P4, 2 – 1 = 1, P5, 9 – 5 = 4
No preemptive Priority
P1, 16 – 10 = 6, P2, 1 – 1 = 0, P3, 18 – 2 = 16, P4, 19 – 1 = 18, P5, 6 – 5 = 1
Round Robin
P1, 19 – 10 = 9, P2, 2 – 1 = 1, P3, 7 – 2 = 5, P4, 4 – 1 = 3, P5, 14 – 5 = 9
        d. Which of the schedules in part a result in the minimal average waiting time (over all processes)?
FCFS: 0+10+11+13+14 = 48/5 = 9.6
SJF: 9+0+2+1+4 = 16/5 = 3.2
No preemptive Priority: 6+0+16+18+1 = 41/5 = 8.2
Round Robin: 9+1+5+3+9 = 27/5 = 5.4
The shortest average waiting time is Shortest Job First (SJF) at an average of 3.2 milliseconds
    20. What advantage is there in having different time-quantum sizes on different levels of a multilevel queuing system? 

Because each queue must have more priority over lower-priority queues in a multi level queuing system, the different time-quantum sizes help in distinguishing between the queues by priority and determining the CPU time slices each queue will have. For instance

    21. Many CPU scheduling algorithms are parameterized. For example, the RR algorithm requires a parameter to indicate the time slice. Multilevel feedback queues require parameters to define the number of queues, the scheduling algorithms for each queue, the criteria used to move processes between queues, and so on. These algorithms are thus really sets of algorithms (for example, the set of RR algorithms for all time slices, and so on). One set of algorithms may include another (for example, the FCFS algorithm is the RR algorithm with an infinite time quantum). What (if any) relation holds between the following pairs of sets of algorithms? 
        a. Priority and SJF 
The SJF algorithm is a special case of the general priority-scheduling algorithm. A priority is associated with each process, and the CPU is allocated to a process with the highest priority. Equal priority processes are scheduled in FCFS order.
An SJF algorithm is simply a priority algorithm where the priority p is simply the inverse of the predicted next CPU burst time. The larger the burst time the lower the priority, and vice versa.
        b. Multilevel feedback queues and FCFS 
Processes in the lowest priority queues are scheduled in a FCFS order
        c. Priority and FCFS 
Equal priority processes are scheduled in a FCFS order.
        d. RR and SJF 
No relationship
    22. Describe the differences among short-term, medium-term, and long-term scheduling.

Short Answer
    • Short-term scheduling - also called "CPU scheduler", allocates CPU time to processes in memory 
    • Medium-term scheduling - swaps processes out of memory and reinstates them, to be run, at a later time, as in a time-sharing system 
    • Long-term scheduling - also called "job scheduler" 
A common difference, other than their defined tasks, is also how frequently each scheduler is called.
Long Answer
Short term scheduling concerns with the allocation of CPU time to processes in order to meet some pre-defined system performance objectives.
Medium term scheduling is essentially concerned with memory management, hence it's very often designed as a part of the memory management subsystem of an OS. Its efficient interaction with the short term scheduler is essential for system performances, especially in virtual memory systems. This is the reason why in paged system the pager process is usually run at a very high (dispatching) priority level. 
Long term scheduling obviously controls the degree of multiprogramming in multitasking systems, following certain policies to decide whether the system can honour a new job submission or, if more than one job is submitted, which of them should be selected. The need for some form of compromise between degree of multiprogramming and throughput seems evident, especially when one considers interactive systems.

    23. Describe with the aid of a diagram the life-cycle of a process. You should describe each of the states that it can be in, and the reasons why it moves between these states. 



As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running.
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    24. What information does the operating system keep in the process control block?

Information associated with each process.
    • Process state
    • Program counter
    • CPU registers
    • CPU scheduling information
    • Memory-management information
    • Accounting information
    • I/O status information

    25. Use an example to explain the need for process synchronization.

Cooperating processes may either share a logical address space (ie. Both code and data), or be allowed to share data through files. Process synchronization is, therefore, needed to prevent data inconsistencies where concurrent access by two processes on shared data may occur.

    26. What are critical section and race conditions?

    • Critical section: A section of code for which mutual exclusion must be implemented

    • Race condition: The situation where several processes access – and manipulate shared data concurrently. The final value of the shared data depends upon which process finishes last.

    27. Describe the purpose of semaphores and how they can be used to solve synchronization problems.

Semaphores are nonnegative integer variables that are used as a flag in and OS. The semaphore signals if and when a resource is free and can be used by a process

Semaphores are synchronization tools that can be used to solve synchronization problems by using the atomic operations: wait (p) and signal (v). For instance if two concurrent processes P1 and P2 with statements S1 and S2 respectively require that S2 be executed after S1 has completed then the scheme can be implemented by allowing P1 and P2 to share a common semaphore synch, initialized to 0, and by inserting statements in P1 to allow waiting and signaling and statements in P2 to wait, the synchronization problem can be solved.


    28. How can semaphores be used to protect critical sections? Which programming errors can occur?

A semaphore is assigned to the resource, it's shared among all processes that need to access the resource, and its counter is initialized to 1. A process then waits on the semaphore upon entering the critical section for the resource, and signals on leaving it. The first process will get access. If another process arrives while the former is still in the critical section, it'll be blocked, and so will further processes. In this situation the absolute value of the counter is equal to the number of processes waiting to enter the critical section. Every process leaving the critical section will let a waiting process use the resource by signaling the semaphore.

Programming errors
    • Starvation
    • Deadlock

    29. How can a message system be used to achieve synchronization?
Messages are yet another form of shared resource, through which cooperating processes can communicate data to each other. intuitively, we can think of implementing them by means of two primitives like 
    • send(dest, msg): to send a message to a designated process, destination of the message; 
    • receive(source, msg): to receive a message from a designated process; 
Synchronization: the primitives can cause the destination or source of the message to block until the delivery is completed (successfully or not). In this case they are called blocking, and nonblocking otherwise. The reception can also be either explicitly confirmed through the use of the analog of a return receipt or not. 
    30. Explain the term deadlock.

A set of blocked processes each holding a resource and waiting to acquire a resource held by another process in the set.

    31. What are resource allocation graphs?
A set of vertices V and a set of edges E.
    • V is partitioned into two types:
        ◦ P = {P1, P2, …,Pn}, the set consisting of all the processes in the system.
        ◦ R = {R1, R2, …,Rm}, the set consisting of all resource types in the system.
    • request edge – directed edge P1 ®Rj
    • assignment edge – directed edge Rj ®Pi

    32. Name the necessary conditions for a deadlock.

Deadlock can arise if four conditions hold simultaneously.

    • Mutual exclusion:  only one process at a time can use a resource.
    • Hold and wait:  a process holding at least one resource is waiting to acquire additional resources held by other processes.
    • No preemption:  a resource can be released only voluntarily by the process holding it, after that process has completed its task.
    • Circular wait:  there exists a set {P0, P1, …, P0} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for a resource that is held by P2, …, Pn–1 is waiting for a resource that is held by 
Pn, and P0 is waiting for a resource that is held by P0.

    33. Can deadlocks be prevented?

Yes, deadlocks can be prevented when you ensure that at least one of the four conditions cannot hold

    • Mutual Exclusion – not required for sharable resources; must hold for nonsharable resources.
    • Hold and Wait – must guarantee that whenever a process requests a resource, it does not hold any other resources.
        ◦ Require process to request and be allocated all its resources before it begins execution, or allow process to request resources only when the process has none.
        ◦ Low resource utilization; starvation possible.
    • No Preemption –
        ◦ If a process that is holding some resources requests another resource that cannot be immediately allocated to it, then all resources currently being held are released.
        ◦ Preempted resources are added to the list of resources for which the process is waiting.
        ◦ Process will be restarted only when it can regain its old resources, as well as the new ones that it is requesting.
    • Circular Wait – impose a total ordering of all resource types, and require that each process requests resources in an increasing order of enumeration.

    34. Is it possible to detect deadlocks? If yes, how can detected deadlocks be resolved?

Yes, deadlocks can be detected using a detection algorithm (Periodically invoke an algorithm that searches for the deadlock), if the resource allocation graphs contains no cycles, then there are no deadlocks, if a graph contains a cycle: i.e. if only one instance per resource type, then there is a deadlock, if several instances per resource type there is a possibility of a deadlock
	
Deadlocks can be resolved in the following ways
    • Abort all deadlocked processes
    • Abort one process at a time until the deadlock cycle is eliminated.

    3. Memory

    35. Describe the following memory allocation algorithms: 
        a. First fit: Allocate the first hole that is big enough. 
        b. Best fit: Allocate the smallest hole that is big enough; must search entire list, unless ordered by size. Produces the smallest leftover hole.
        c. Worst fit: Allocate the largest hole; must also search entire list. Produces the largest leftover hole.
    36. Explain the term “swapping”.

Swapping is a mechanism of removing a process temporarily out of memory to a backing store and then, and then brought back into memory for continued execution.

Example: Assume a multiprogramming environment with a round robin CPU scheduling algorithm. When a quantum expires, the memory manager will start to swap out the process that just finished, and swap in another process to the memory space that has just been freed.


    37. What is the difference between internal and external fragmentation?

    • Internal Fragmentation – allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used
    • External Fragmentation – total memory space exists to satisfy a request, but it is not contiguous; storage is fragmented into a large number of small holes

    38. Explain the term “segmentation”.

    • Memory-management scheme that supports user view of memory. 

    39. What is compaction and in which context is it used?

Compaction is referred to as garbage collection, performed by the OS to reclaim fragmented sections of the memory space. 



Its should be used when;
    • When a certain percentage of memory becomes busy, say 80%.  
    • When there are processes waiting to get in. 
    • After a prescribed amount of time has elapsed

    40. Explain the term “paging”.

Paging is a memory management scheme that permits the physical address space of a process to be noncontiguous.???

    41. What are page faults? How are they handled by an operating system?

Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 



    42. Explain the term “virtual memory”.

A technique that allows programs to be executed even though they are not stored entirely in memory. It gives a user the illusion that a large amount of main memory is available when, in fact, it is not. 

    43. Name three loading and three replacement strategies and discuss their advantages and disadvantages.













    44. Why are segmentation and paging sometimes combined into one scheme? 

To remove the problems of external fragmentation and compaction, because the pages are of fixed length

    45. In the context of memory management, under which circumstances do external and internal fragmentations occur? How can each be handled? 

Internal Fragmentation occurs when fixed partitions suffer from inefficient memory use - any process, no matter how small, occupies an entire partition.

Internal fragmentation can be solved by combining free areas of memory whenever possible and combining it to form a block that will be deallocate.

External Fragmentation occurs when process are loaded and removed from memory, the free memory space is broken down into little pieces. 

External fragmentation can be handle by compaction; the OS reclaims fragmented sections of the memory space and compacts them to make one large block of memory that is large enough to accommodate some or all of the jobs waiting to get in. 


    46. Under what circumstances do page faults occur? Describe the actions taken by the operating system when a page fault occurs. 
Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 

    47. When virtual memory is implemented in a computing system, it carries certain costs and certain benefits. List those costs and the benefits. It is possible for the costs to exceed the benefits. Explain what measures you can take to ensure that this imbalance does not occur.

Benefits
    • Users can write extremely very large programs without being concerned with the limitations of main memory. 
    • Increases multiprogramming with a corresponding increase in CPU utilization and throughput, but with no increase in response time or turnaround time. 
    • Less I/O would be needed to load or swap each user program into memory, so each user program would run faster. 

    48. What is the cause of thrashing? How does the system detect thrashing? Once it detects thrashing, what can the system do to eliminate this problem? 

Thrashing is caused when a process is busy swapping pages in and out. The process spend more time paging than executing

Thrashing can be detected by;
    • Low CPU utilization

Thrashing can be limited by using a local replacement algorithm; if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash also.

    49. What is meant by the term demand paging in a virtual memory management system, and how is it implemented?

Demand paging is swapping a process into memory when it is needed

Implementation
    • Page is needed Þ reference to it
    • invalid reference Þ abort
    • not-in-memory Þ bring to memory
    1. Introduction 
    1. Describe the main purpose of an operating system.

The main purpose of an OS is to act as an intermediary between the user of a computer and the computer hardware by providing an environment in which a user can execute programs in a convenient and efficient manner.

    2. Identify 3 activities of an operating system in respect to process management, memory management, I/O management and file management. 
	Process Management:
        ◦ Process creation and deletion.
        ◦ Process suspension and resumption.
        ◦ Provision of mechanisms for:
            i. process synchronization
            ii. process communication
	Memory Management:
                • Keep track of which parts of memory are currently being used and by whom.
                • Decide which processes to load when memory space becomes available.
                • Allocate and deallocate memory space as needed.
	File Management:
    • File creation and deletion.
    • Directory creation and deletion.
    • Support of primitives for manipulating files and directories.
    • Mapping files onto secondary storage.
    • File backup on stable (nonvolatile) storage media.
	I/O Management:
        ◦ A buffer-caching system 
        ◦ A general device-driver interface
        ◦ Drivers for specific hardware devices
	Secondary Storage Management:
    • Free space management
    • Storage allocation
    • Disk scheduling
	
    3. Name two storage media with random access and two media with sequential access.
	Random Access:
    • RAM
    • Registers
    • Cache
	Sequential Access:
    • Magnetic Disks???
    • Optical Disks
    • Magnetic Tapes

    4. What are the differences between a trap and an interrupt? What is the use of each function? 

	A trap is a software-generated interrupt caused either by an error or a specific 	user program request.
An Interrupt is an asynchronous break in program flow that occurs as a result of events outside the running program. It’s usually hardware related, stemming from events such as a button press, timer expiration, or completion of a data transfer. An Interrupt triggers execution of instructions that perform work on behalf of the system, but not necessarily the current program. They were developed for exception handling and were later applied to I/O events.
    5. What are the purposes of system calls and system programs?

System calls provide an interface between the process and the operating system. System calls allow user-level processes to request some services from the operating system which process itself is not allowed to do. For example, for I/O a process involves a system call telling the operating system to read or write particular area and this request is satisfied by the operating system.
	
The purpose of a system program is to provide a convenient environment for   program development and execution.  


    6. What are the main advantages of multiprogramming?

    • Increases CPU utilization by organizing  jobs so that the CPU always has one to execute
    • Allows time sharing: many users can use a computer system interactively at the same time

	
    7. What is multi tasking, multi programming, multi threading? 

Multi tasking: The ability to execute more than one task at the same time

Multi programming: The ability to run several programs on a uniprocessor.

Multi threading:The ability of an operating system to execute different parts of a program, called threads, simultaneously. ***??

    8. What is a Real-Time System? 

A real-time system is one with well defined, fixed time constraints whereby processing must be done within the defined constraints or the system will fail


    9. What is the difference between Hard and Soft real-time systems? What is a mission critical system?
    • Hard real-time
        ◦ Secondary storage limited or absent, data stored in short term memory, or read-only memory (ROM)
        ◦ Conflicts with time-sharing systems, not supported by general-purpose operating systems.
        ◦ Guarantees that all critical tasks be completed on time
    • Soft real-time
        ◦ Limited utility in industrial control of robotics
        ◦ Useful in applications (multimedia, virtual reality) requiring advanced operating-system features.
        ◦ A critical real-time task gets priority over other tasks and retains that priority until it completes

    • Mission Critical System: 
	


    2. Processes

    10. Explain the difference between a program and a process.

A process is a program loaded into memory and executing. It is a unit of work in a system. 

A program is lines of code or algorithm expressed in some suitable notation.

    11. What are typical process states and typical transitions between these states.

	As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • Waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.



 


Transitions between the states

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → Waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out  (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running. 
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    12. Why are program counter and processor registers part of the process control block?
The process control block represents each process in the O/S. 

    • The program counter contains the address of the next instruction for execution for 	this process
    • The processor registers are a volatile storage medium that are used to store the current state of a process when an interrupt occurs. They allow the process to be 	continued correctly afterwards 

    13. Explain the main differences between processes and threads.
Differences
    • Unlike processes, threads are not independent of one another. 
    • Unlike processes, all threads can access every address in the task. 
    • Unlike processes, threads are designed to assist one other. Note that processes might or might not assist one another because processes may originate from different users. 
	Similarities
    • Like processes threads share CPU and only one thread active (running) at a time. 
    • Like processes, threads within a process, execute sequentially. 
    • Like processes, threads can create children. 
    • And like process, if one thread is blocked, another thread can run. 

    14. What are the requirements for a good scheduler?

    • Max CPU utilization – keep the CPU as busy as possible
    • Max Throughput – Number of processes that complete their execution per time unit
    • Min Turnaround time – amount of time to execute a particular process
    • Min Waiting time – amount of time a process has been waiting in the ready queue
    • Min Response time – amount of time it takes from when a request was submitted until the first response is produced, not output  (for time-sharing environment)

    15. Explain the difference between preemptive and non-preemptive schedulers.

    • Nonpreemptive – once CPU given to the process it cannot be interrupted until it completes its CPU burst or switches to a waiting state.
    • Preemptive – if a new process arrives with CPU burst length less than remaining time of current executing process, preempt.  This scheme is known as the Shortest-Remaining-Time-First (SRTF).

Explain why strict nonpreemptive scheduling is unlikely to be used in a computer center.
	Convoy effect, short jobs are made to wait by longer jobs

    16. Explain the scheduling strategies FCFS, LPTF, SJF, SRTF, and RR.

    • FCFS: (First Come First Served) process that requests the CPU first, gets the CPU first (FIFO)

    • SJF: (Shortest Job First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the smallest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    • SRTF: (Shortest Remaining Time First) pre-emptive SJF scheduling: When a process with a shorter next CPU burst than what is left of the currently executing process arrives in the ready queue, it will preempt the currently executing process

    • RR: (Round Robin scheduling) designed especially for timesharing systems. Similar to FCFS but preemption is added to switch between processes. A small unit called a time quantum or time slice is defined. The ready queue is treated as a circular queue. The CPU scheduler goes around the ready queue allocating the CPU for each process for a time interval of up to one time quantum


    • LPTF: (Long Processing Time First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the longest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    17. What are two differences between user-level threads and kernel-level threads? Under what circumstances is one type better than the another?

    • User-level threads are faster to create and require less overhead than kernel-level threads.
    • The kernel knows about kernel-level threads, but not user-level threads, so K/L threads can be scheduled by the kernel’s scheduler.

K/L threads are better than U/L threads when you will have threads that block, for example, on I/O. In these cases, the scheduler can choose another K/L to run so that the whole process doesn’t block.

U/L threads are better when there will be a lot of them created, or when they will be created and destroyed relatively quickly because there is little creation/destruction overhead. They are also better when the threads need to communicate because thread communication is simply a procedure call in U/L threads, which is faster than going through the kernel.

Short Answer
    • User threads are built using a library and are unknown or undifferentiated by the kernel. 
    • Kernel threads require more operating system resources because they are managed by the kernel. 
A user thread is more appropriate for low-level tasks, whereas a kernel thread is better for high-priority tasks that should get high priority to system resources

    18. What resources are used when a thread is created? How do they differ from those used when a process is created?

When a thread is created, it needs its own execution state. This basically involves allocating it memory for its stack, and giving it a PC, SP, and storage for its other registers, all within the processes address space.

When a process is created, it receives an entirely new address space, which means that the system has to create a new mapping from addresses to physical memory, and it has to allocate the process a new block of physical memory. Within that address space, the system must keep track of the open files, the process’s permissions, etc. It must also allocate things that are required for each thread within that process to have its own execution state, including a place in memory to store the stack and the registers for each thread in the process, as listed above.

So the resources used in creating a thread are a strict (and much smaller) subset of those used when creating a process.

Short Answer
When created, a thread requires an ID, a register set, a stack and program counter. 
Threads are smaller than processes. In addition to the thread information, each process creates its own Process Control Block (PCB), containing a code section, data section, and O/S resources. Threads reference these items from their creating or parent process.

    19. Consider the following set of processes, with the length of the CPU-burst time given in milliseconds: 
Process
Burst Time 
Priority
P1
10
3
P2
1
1
P3
2
3
P4
1
4
P5
5
2
The processes are assumed to have arrived in the order PI, P2, P3, P4, P5, all at time 0 
        a. Draw four Gantt charts that illustrate the execution of these processes using FCFS, SJF, a nonpreemptive priority (a smaller priority number implies a higher priority), and RR (quantum = 1) scheduling.
FCFS
P1
P2
P3
P4
P5
	          0			           10     11              13     14                             19
		      SJF	
P2
P4
P3
P5
P1
		           0      1	     2                4                               9                                              19


No preemptive Priority
P2
P5
P1
P3
P4
     0      1                             6                                       16                    18    19
Round Robin
P1
P2
P3
P4
P5
P1
P3
P5
P1
P5
P1
P5
P1
P5
P1
P1
P1
P1
P1
0     1      2     3      4      5      6      7      8      9     10   11   12     13   14     15    16   17    18      19

        b. What is the turnaround time of each process for each of the scheduling algorithms in part a?
Turnaround time = Completion time – Arrival time 
FCFS
P1, 10 – 0 = 10, P2, 11 – 0 = 11, P3, 13 – 0 = 13, P4, 14 – 0 = 14, P5, 19 – 0 = 19
SJF 
P1, 19 – 0 = 19, P2, 1 – 0 = 1, P3, 4 – 0 = 4, P4, 2 – 0 = 2, P5, 9 – 0 = 9
No preemptive Priority
P1, 16 – 0 = 16, P2, 1 – 0 = 1, P3, 18 – 0 = 18, P4, 19 – 0 = 19, P5, 6 – 0 = 6
Round Robin
P1, 19 – 0 = 19, P2, 2 – 0 = 2, P3, 7 – 0 = 7, P4, 4 – 0 = 4, P5, 14 – 0 = 14
        c. What is the waiting time of each process for each of the scheduling algorithms in part a?
Waiting time = Turnaround time – Burst time
FCFS
P1, 10 – 10 = 0, P2, 11 – 1 = 10, P3, 13 – 2 = 11, P4, 14 – 1 = 13, P5, 19 – 5 = 14
SJF 
P1, 19 – 10 = 9, P2, 1 – 1 = 0, P3, 4 – 2 = 2, P4, 2 – 1 = 1, P5, 9 – 5 = 4
No preemptive Priority
P1, 16 – 10 = 6, P2, 1 – 1 = 0, P3, 18 – 2 = 16, P4, 19 – 1 = 18, P5, 6 – 5 = 1
Round Robin
P1, 19 – 10 = 9, P2, 2 – 1 = 1, P3, 7 – 2 = 5, P4, 4 – 1 = 3, P5, 14 – 5 = 9
        d. Which of the schedules in part a result in the minimal average waiting time (over all processes)?
FCFS: 0+10+11+13+14 = 48/5 = 9.6
SJF: 9+0+2+1+4 = 16/5 = 3.2
No preemptive Priority: 6+0+16+18+1 = 41/5 = 8.2
Round Robin: 9+1+5+3+9 = 27/5 = 5.4
The shortest average waiting time is Shortest Job First (SJF) at an average of 3.2 milliseconds
    20. What advantage is there in having different time-quantum sizes on different levels of a multilevel queuing system? 

Because each queue must have more priority over lower-priority queues in a multi level queuing system, the different time-quantum sizes help in distinguishing between the queues by priority and determining the CPU time slices each queue will have. For instance

    21. Many CPU scheduling algorithms are parameterized. For example, the RR algorithm requires a parameter to indicate the time slice. Multilevel feedback queues require parameters to define the number of queues, the scheduling algorithms for each queue, the criteria used to move processes between queues, and so on. These algorithms are thus really sets of algorithms (for example, the set of RR algorithms for all time slices, and so on). One set of algorithms may include another (for example, the FCFS algorithm is the RR algorithm with an infinite time quantum). What (if any) relation holds between the following pairs of sets of algorithms? 
        a. Priority and SJF 
The SJF algorithm is a special case of the general priority-scheduling algorithm. A priority is associated with each process, and the CPU is allocated to a process with the highest priority. Equal priority processes are scheduled in FCFS order.
An SJF algorithm is simply a priority algorithm where the priority p is simply the inverse of the predicted next CPU burst time. The larger the burst time the lower the priority, and vice versa.
        b. Multilevel feedback queues and FCFS 
Processes in the lowest priority queues are scheduled in a FCFS order
        c. Priority and FCFS 
Equal priority processes are scheduled in a FCFS order.
        d. RR and SJF 
No relationship
    22. Describe the differences among short-term, medium-term, and long-term scheduling.

Short Answer
    • Short-term scheduling - also called "CPU scheduler", allocates CPU time to processes in memory 
    • Medium-term scheduling - swaps processes out of memory and reinstates them, to be run, at a later time, as in a time-sharing system 
    • Long-term scheduling - also called "job scheduler" 
A common difference, other than their defined tasks, is also how frequently each scheduler is called.
Long Answer
Short term scheduling concerns with the allocation of CPU time to processes in order to meet some pre-defined system performance objectives.
Medium term scheduling is essentially concerned with memory management, hence it's very often designed as a part of the memory management subsystem of an OS. Its efficient interaction with the short term scheduler is essential for system performances, especially in virtual memory systems. This is the reason why in paged system the pager process is usually run at a very high (dispatching) priority level. 
Long term scheduling obviously controls the degree of multiprogramming in multitasking systems, following certain policies to decide whether the system can honour a new job submission or, if more than one job is submitted, which of them should be selected. The need for some form of compromise between degree of multiprogramming and throughput seems evident, especially when one considers interactive systems.

    23. Describe with the aid of a diagram the life-cycle of a process. You should describe each of the states that it can be in, and the reasons why it moves between these states. 



As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running.
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    24. What information does the operating system keep in the process control block?

Information associated with each process.
    • Process state
    • Program counter
    • CPU registers
    • CPU scheduling information
    • Memory-management information
    • Accounting information
    • I/O status information

    25. Use an example to explain the need for process synchronization.

Cooperating processes may either share a logical address space (ie. Both code and data), or be allowed to share data through files. Process synchronization is, therefore, needed to prevent data inconsistencies where concurrent access by two processes on shared data may occur.

    26. What are critical section and race conditions?

    • Critical section: A section of code for which mutual exclusion must be implemented

    • Race condition: The situation where several processes access – and manipulate shared data concurrently. The final value of the shared data depends upon which process finishes last.

    27. Describe the purpose of semaphores and how they can be used to solve synchronization problems.

Semaphores are nonnegative integer variables that are used as a flag in and OS. The semaphore signals if and when a resource is free and can be used by a process

Semaphores are synchronization tools that can be used to solve synchronization problems by using the atomic operations: wait (p) and signal (v). For instance if two concurrent processes P1 and P2 with statements S1 and S2 respectively require that S2 be executed after S1 has completed then the scheme can be implemented by allowing P1 and P2 to share a common semaphore synch, initialized to 0, and by inserting statements in P1 to allow waiting and signaling and statements in P2 to wait, the synchronization problem can be solved.


    28. How can semaphores be used to protect critical sections? Which programming errors can occur?

A semaphore is assigned to the resource, it's shared among all processes that need to access the resource, and its counter is initialized to 1. A process then waits on the semaphore upon entering the critical section for the resource, and signals on leaving it. The first process will get access. If another process arrives while the former is still in the critical section, it'll be blocked, and so will further processes. In this situation the absolute value of the counter is equal to the number of processes waiting to enter the critical section. Every process leaving the critical section will let a waiting process use the resource by signaling the semaphore.

Programming errors
    • Starvation
    • Deadlock

    29. How can a message system be used to achieve synchronization?
Messages are yet another form of shared resource, through which cooperating processes can communicate data to each other. intuitively, we can think of implementing them by means of two primitives like 
    • send(dest, msg): to send a message to a designated process, destination of the message; 
    • receive(source, msg): to receive a message from a designated process; 
Synchronization: the primitives can cause the destination or source of the message to block until the delivery is completed (successfully or not). In this case they are called blocking, and nonblocking otherwise. The reception can also be either explicitly confirmed through the use of the analog of a return receipt or not. 
    30. Explain the term deadlock.

A set of blocked processes each holding a resource and waiting to acquire a resource held by another process in the set.

    31. What are resource allocation graphs?
A set of vertices V and a set of edges E.
    • V is partitioned into two types:
        ◦ P = {P1, P2, …,Pn}, the set consisting of all the processes in the system.
        ◦ R = {R1, R2, …,Rm}, the set consisting of all resource types in the system.
    • request edge – directed edge P1 ®Rj
    • assignment edge – directed edge Rj ®Pi

    32. Name the necessary conditions for a deadlock.

Deadlock can arise if four conditions hold simultaneously.

    • Mutual exclusion:  only one process at a time can use a resource.
    • Hold and wait:  a process holding at least one resource is waiting to acquire additional resources held by other processes.
    • No preemption:  a resource can be released only voluntarily by the process holding it, after that process has completed its task.
    • Circular wait:  there exists a set {P0, P1, …, P0} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for a resource that is held by P2, …, Pn–1 is waiting for a resource that is held by 
Pn, and P0 is waiting for a resource that is held by P0.

    33. Can deadlocks be prevented?

Yes, deadlocks can be prevented when you ensure that at least one of the four conditions cannot hold

    • Mutual Exclusion – not required for sharable resources; must hold for nonsharable resources.
    • Hold and Wait – must guarantee that whenever a process requests a resource, it does not hold any other resources.
        ◦ Require process to request and be allocated all its resources before it begins execution, or allow process to request resources only when the process has none.
        ◦ Low resource utilization; starvation possible.
    • No Preemption –
        ◦ If a process that is holding some resources requests another resource that cannot be immediately allocated to it, then all resources currently being held are released.
        ◦ Preempted resources are added to the list of resources for which the process is waiting.
        ◦ Process will be restarted only when it can regain its old resources, as well as the new ones that it is requesting.
    • Circular Wait – impose a total ordering of all resource types, and require that each process requests resources in an increasing order of enumeration.

    34. Is it possible to detect deadlocks? If yes, how can detected deadlocks be resolved?

Yes, deadlocks can be detected using a detection algorithm (Periodically invoke an algorithm that searches for the deadlock), if the resource allocation graphs contains no cycles, then there are no deadlocks, if a graph contains a cycle: i.e. if only one instance per resource type, then there is a deadlock, if several instances per resource type there is a possibility of a deadlock
	
Deadlocks can be resolved in the following ways
    • Abort all deadlocked processes
    • Abort one process at a time until the deadlock cycle is eliminated.

    3. Memory

    35. Describe the following memory allocation algorithms: 
        a. First fit: Allocate the first hole that is big enough. 
        b. Best fit: Allocate the smallest hole that is big enough; must search entire list, unless ordered by size. Produces the smallest leftover hole.
        c. Worst fit: Allocate the largest hole; must also search entire list. Produces the largest leftover hole.
    36. Explain the term “swapping”.

Swapping is a mechanism of removing a process temporarily out of memory to a backing store and then, and then brought back into memory for continued execution.

Example: Assume a multiprogramming environment with a round robin CPU scheduling algorithm. When a quantum expires, the memory manager will start to swap out the process that just finished, and swap in another process to the memory space that has just been freed.


    37. What is the difference between internal and external fragmentation?

    • Internal Fragmentation – allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used
    • External Fragmentation – total memory space exists to satisfy a request, but it is not contiguous; storage is fragmented into a large number of small holes

    38. Explain the term “segmentation”.

    • Memory-management scheme that supports user view of memory. 

    39. What is compaction and in which context is it used?

Compaction is referred to as garbage collection, performed by the OS to reclaim fragmented sections of the memory space. 



Its should be used when;
    • When a certain percentage of memory becomes busy, say 80%.  
    • When there are processes waiting to get in. 
    • After a prescribed amount of time has elapsed

    40. Explain the term “paging”.

Paging is a memory management scheme that permits the physical address space of a process to be noncontiguous.???

    41. What are page faults? How are they handled by an operating system?

Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 



    42. Explain the term “virtual memory”.

A technique that allows programs to be executed even though they are not stored entirely in memory. It gives a user the illusion that a large amount of main memory is available when, in fact, it is not. 

    43. Name three loading and three replacement strategies and discuss their advantages and disadvantages.













    44. Why are segmentation and paging sometimes combined into one scheme? 

To remove the problems of external fragmentation and compaction, because the pages are of fixed length

    45. In the context of memory management, under which circumstances do external and internal fragmentations occur? How can each be handled? 

Internal Fragmentation occurs when fixed partitions suffer from inefficient memory use - any process, no matter how small, occupies an entire partition.

Internal fragmentation can be solved by combining free areas of memory whenever possible and combining it to form a block that will be deallocate.

External Fragmentation occurs when process are loaded and removed from memory, the free memory space is broken down into little pieces. 

External fragmentation can be handle by compaction; the OS reclaims fragmented sections of the memory space and compacts them to make one large block of memory that is large enough to accommodate some or all of the jobs waiting to get in. 


    46. Under what circumstances do page faults occur? Describe the actions taken by the operating system when a page fault occurs. 
Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 

    47. When virtual memory is implemented in a computing system, it carries certain costs and certain benefits. List those costs and the benefits. It is possible for the costs to exceed the benefits. Explain what measures you can take to ensure that this imbalance does not occur.

Benefits
    • Users can write extremely very large programs without being concerned with the limitations of main memory. 
    • Increases multiprogramming with a corresponding increase in CPU utilization and throughput, but with no increase in response time or turnaround time. 
    • Less I/O would be needed to load or swap each user program into memory, so each user program would run faster. 

    48. What is the cause of thrashing? How does the system detect thrashing? Once it detects thrashing, what can the system do to eliminate this problem? 

Thrashing is caused when a process is busy swapping pages in and out. The process spend more time paging than executing

Thrashing can be detected by;
    • Low CPU utilization

Thrashing can be limited by using a local replacement algorithm; if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash also.

    49. What is meant by the term demand paging in a virtual memory management system, and how is it implemented?

Demand paging is swapping a process into memory when it is needed

Implementation
    • Page is needed Þ reference to it
    • invalid reference Þ abort
    • not-in-memory Þ bring to memory
    1. Introduction 
    1. Describe the main purpose of an operating system.

The main purpose of an OS is to act as an intermediary between the user of a computer and the computer hardware by providing an environment in which a user can execute programs in a convenient and efficient manner.

    2. Identify 3 activities of an operating system in respect to process management, memory management, I/O management and file management. 
	Process Management:
        ◦ Process creation and deletion.
        ◦ Process suspension and resumption.
        ◦ Provision of mechanisms for:
            i. process synchronization
            ii. process communication
	Memory Management:
                • Keep track of which parts of memory are currently being used and by whom.
                • Decide which processes to load when memory space becomes available.
                • Allocate and deallocate memory space as needed.
	File Management:
    • File creation and deletion.
    • Directory creation and deletion.
    • Support of primitives for manipulating files and directories.
    • Mapping files onto secondary storage.
    • File backup on stable (nonvolatile) storage media.
	I/O Management:
        ◦ A buffer-caching system 
        ◦ A general device-driver interface
        ◦ Drivers for specific hardware devices
	Secondary Storage Management:
    • Free space management
    • Storage allocation
    • Disk scheduling
	
    3. Name two storage media with random access and two media with sequential access.
	Random Access:
    • RAM
    • Registers
    • Cache
	Sequential Access:
    • Magnetic Disks???
    • Optical Disks
    • Magnetic Tapes

    4. What are the differences between a trap and an interrupt? What is the use of each function? 

	A trap is a software-generated interrupt caused either by an error or a specific 	user program request.
An Interrupt is an asynchronous break in program flow that occurs as a result of events outside the running program. It’s usually hardware related, stemming from events such as a button press, timer expiration, or completion of a data transfer. An Interrupt triggers execution of instructions that perform work on behalf of the system, but not necessarily the current program. They were developed for exception handling and were later applied to I/O events.
    5. What are the purposes of system calls and system programs?

System calls provide an interface between the process and the operating system. System calls allow user-level processes to request some services from the operating system which process itself is not allowed to do. For example, for I/O a process involves a system call telling the operating system to read or write particular area and this request is satisfied by the operating system.
	
The purpose of a system program is to provide a convenient environment for   program development and execution.  


    6. What are the main advantages of multiprogramming?

    • Increases CPU utilization by organizing  jobs so that the CPU always has one to execute
    • Allows time sharing: many users can use a computer system interactively at the same time

	
    7. What is multi tasking, multi programming, multi threading? 

Multi tasking: The ability to execute more than one task at the same time

Multi programming: The ability to run several programs on a uniprocessor.

Multi threading:The ability of an operating system to execute different parts of a program, called threads, simultaneously. ***??

    8. What is a Real-Time System? 

A real-time system is one with well defined, fixed time constraints whereby processing must be done within the defined constraints or the system will fail


    9. What is the difference between Hard and Soft real-time systems? What is a mission critical system?
    • Hard real-time
        ◦ Secondary storage limited or absent, data stored in short term memory, or read-only memory (ROM)
        ◦ Conflicts with time-sharing systems, not supported by general-purpose operating systems.
        ◦ Guarantees that all critical tasks be completed on time
    • Soft real-time
        ◦ Limited utility in industrial control of robotics
        ◦ Useful in applications (multimedia, virtual reality) requiring advanced operating-system features.
        ◦ A critical real-time task gets priority over other tasks and retains that priority until it completes

    • Mission Critical System: 
	


    2. Processes

    10. Explain the difference between a program and a process.

A process is a program loaded into memory and executing. It is a unit of work in a system. 

A program is lines of code or algorithm expressed in some suitable notation.

    11. What are typical process states and typical transitions between these states.

	As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • Waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.



 


Transitions between the states

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → Waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out  (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running. 
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    12. Why are program counter and processor registers part of the process control block?
The process control block represents each process in the O/S. 

    • The program counter contains the address of the next instruction for execution for 	this process
    • The processor registers are a volatile storage medium that are used to store the current state of a process when an interrupt occurs. They allow the process to be 	continued correctly afterwards 

    13. Explain the main differences between processes and threads.
Differences
    • Unlike processes, threads are not independent of one another. 
    • Unlike processes, all threads can access every address in the task. 
    • Unlike processes, threads are designed to assist one other. Note that processes might or might not assist one another because processes may originate from different users. 
	Similarities
    • Like processes threads share CPU and only one thread active (running) at a time. 
    • Like processes, threads within a process, execute sequentially. 
    • Like processes, threads can create children. 
    • And like process, if one thread is blocked, another thread can run. 

    14. What are the requirements for a good scheduler?

    • Max CPU utilization – keep the CPU as busy as possible
    • Max Throughput – Number of processes that complete their execution per time unit
    • Min Turnaround time – amount of time to execute a particular process
    • Min Waiting time – amount of time a process has been waiting in the ready queue
    • Min Response time – amount of time it takes from when a request was submitted until the first response is produced, not output  (for time-sharing environment)

    15. Explain the difference between preemptive and non-preemptive schedulers.

    • Nonpreemptive – once CPU given to the process it cannot be interrupted until it completes its CPU burst or switches to a waiting state.
    • Preemptive – if a new process arrives with CPU burst length less than remaining time of current executing process, preempt.  This scheme is known as the Shortest-Remaining-Time-First (SRTF).

Explain why strict nonpreemptive scheduling is unlikely to be used in a computer center.
	Convoy effect, short jobs are made to wait by longer jobs

    16. Explain the scheduling strategies FCFS, LPTF, SJF, SRTF, and RR.

    • FCFS: (First Come First Served) process that requests the CPU first, gets the CPU first (FIFO)

    • SJF: (Shortest Job First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the smallest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    • SRTF: (Shortest Remaining Time First) pre-emptive SJF scheduling: When a process with a shorter next CPU burst than what is left of the currently executing process arrives in the ready queue, it will preempt the currently executing process

    • RR: (Round Robin scheduling) designed especially for timesharing systems. Similar to FCFS but preemption is added to switch between processes. A small unit called a time quantum or time slice is defined. The ready queue is treated as a circular queue. The CPU scheduler goes around the ready queue allocating the CPU for each process for a time interval of up to one time quantum


    • LPTF: (Long Processing Time First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the longest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    17. What are two differences between user-level threads and kernel-level threads? Under what circumstances is one type better than the another?

    • User-level threads are faster to create and require less overhead than kernel-level threads.
    • The kernel knows about kernel-level threads, but not user-level threads, so K/L threads can be scheduled by the kernel’s scheduler.

K/L threads are better than U/L threads when you will have threads that block, for example, on I/O. In these cases, the scheduler can choose another K/L to run so that the whole process doesn’t block.

U/L threads are better when there will be a lot of them created, or when they will be created and destroyed relatively quickly because there is little creation/destruction overhead. They are also better when the threads need to communicate because thread communication is simply a procedure call in U/L threads, which is faster than going through the kernel.

Short Answer
    • User threads are built using a library and are unknown or undifferentiated by the kernel. 
    • Kernel threads require more operating system resources because they are managed by the kernel. 
A user thread is more appropriate for low-level tasks, whereas a kernel thread is better for high-priority tasks that should get high priority to system resources

    18. What resources are used when a thread is created? How do they differ from those used when a process is created?

When a thread is created, it needs its own execution state. This basically involves allocating it memory for its stack, and giving it a PC, SP, and storage for its other registers, all within the processes address space.

When a process is created, it receives an entirely new address space, which means that the system has to create a new mapping from addresses to physical memory, and it has to allocate the process a new block of physical memory. Within that address space, the system must keep track of the open files, the process’s permissions, etc. It must also allocate things that are required for each thread within that process to have its own execution state, including a place in memory to store the stack and the registers for each thread in the process, as listed above.

So the resources used in creating a thread are a strict (and much smaller) subset of those used when creating a process.

Short Answer
When created, a thread requires an ID, a register set, a stack and program counter. 
Threads are smaller than processes. In addition to the thread information, each process creates its own Process Control Block (PCB), containing a code section, data section, and O/S resources. Threads reference these items from their creating or parent process.

    19. Consider the following set of processes, with the length of the CPU-burst time given in milliseconds: 
Process
Burst Time 
Priority
P1
10
3
P2
1
1
P3
2
3
P4
1
4
P5
5
2
The processes are assumed to have arrived in the order PI, P2, P3, P4, P5, all at time 0 
        a. Draw four Gantt charts that illustrate the execution of these processes using FCFS, SJF, a nonpreemptive priority (a smaller priority number implies a higher priority), and RR (quantum = 1) scheduling.
FCFS
P1
P2
P3
P4
P5
	          0			           10     11              13     14                             19
		      SJF	
P2
P4
P3
P5
P1
		           0      1	     2                4                               9                                              19


No preemptive Priority
P2
P5
P1
P3
P4
     0      1                             6                                       16                    18    19
Round Robin
P1
P2
P3
P4
P5
P1
P3
P5
P1
P5
P1
P5
P1
P5
P1
P1
P1
P1
P1
0     1      2     3      4      5      6      7      8      9     10   11   12     13   14     15    16   17    18      19

        b. What is the turnaround time of each process for each of the scheduling algorithms in part a?
Turnaround time = Completion time – Arrival time 
FCFS
P1, 10 – 0 = 10, P2, 11 – 0 = 11, P3, 13 – 0 = 13, P4, 14 – 0 = 14, P5, 19 – 0 = 19
SJF 
P1, 19 – 0 = 19, P2, 1 – 0 = 1, P3, 4 – 0 = 4, P4, 2 – 0 = 2, P5, 9 – 0 = 9
No preemptive Priority
P1, 16 – 0 = 16, P2, 1 – 0 = 1, P3, 18 – 0 = 18, P4, 19 – 0 = 19, P5, 6 – 0 = 6
Round Robin
P1, 19 – 0 = 19, P2, 2 – 0 = 2, P3, 7 – 0 = 7, P4, 4 – 0 = 4, P5, 14 – 0 = 14
        c. What is the waiting time of each process for each of the scheduling algorithms in part a?
Waiting time = Turnaround time – Burst time
FCFS
P1, 10 – 10 = 0, P2, 11 – 1 = 10, P3, 13 – 2 = 11, P4, 14 – 1 = 13, P5, 19 – 5 = 14
SJF 
P1, 19 – 10 = 9, P2, 1 – 1 = 0, P3, 4 – 2 = 2, P4, 2 – 1 = 1, P5, 9 – 5 = 4
No preemptive Priority
P1, 16 – 10 = 6, P2, 1 – 1 = 0, P3, 18 – 2 = 16, P4, 19 – 1 = 18, P5, 6 – 5 = 1
Round Robin
P1, 19 – 10 = 9, P2, 2 – 1 = 1, P3, 7 – 2 = 5, P4, 4 – 1 = 3, P5, 14 – 5 = 9
        d. Which of the schedules in part a result in the minimal average waiting time (over all processes)?
FCFS: 0+10+11+13+14 = 48/5 = 9.6
SJF: 9+0+2+1+4 = 16/5 = 3.2
No preemptive Priority: 6+0+16+18+1 = 41/5 = 8.2
Round Robin: 9+1+5+3+9 = 27/5 = 5.4
The shortest average waiting time is Shortest Job First (SJF) at an average of 3.2 milliseconds
    20. What advantage is there in having different time-quantum sizes on different levels of a multilevel queuing system? 

Because each queue must have more priority over lower-priority queues in a multi level queuing system, the different time-quantum sizes help in distinguishing between the queues by priority and determining the CPU time slices each queue will have. For instance

    21. Many CPU scheduling algorithms are parameterized. For example, the RR algorithm requires a parameter to indicate the time slice. Multilevel feedback queues require parameters to define the number of queues, the scheduling algorithms for each queue, the criteria used to move processes between queues, and so on. These algorithms are thus really sets of algorithms (for example, the set of RR algorithms for all time slices, and so on). One set of algorithms may include another (for example, the FCFS algorithm is the RR algorithm with an infinite time quantum). What (if any) relation holds between the following pairs of sets of algorithms? 
        a. Priority and SJF 
The SJF algorithm is a special case of the general priority-scheduling algorithm. A priority is associated with each process, and the CPU is allocated to a process with the highest priority. Equal priority processes are scheduled in FCFS order.
An SJF algorithm is simply a priority algorithm where the priority p is simply the inverse of the predicted next CPU burst time. The larger the burst time the lower the priority, and vice versa.
        b. Multilevel feedback queues and FCFS 
Processes in the lowest priority queues are scheduled in a FCFS order
        c. Priority and FCFS 
Equal priority processes are scheduled in a FCFS order.
        d. RR and SJF 
No relationship
    22. Describe the differences among short-term, medium-term, and long-term scheduling.

Short Answer
    • Short-term scheduling - also called "CPU scheduler", allocates CPU time to processes in memory 
    • Medium-term scheduling - swaps processes out of memory and reinstates them, to be run, at a later time, as in a time-sharing system 
    • Long-term scheduling - also called "job scheduler" 
A common difference, other than their defined tasks, is also how frequently each scheduler is called.
Long Answer
Short term scheduling concerns with the allocation of CPU time to processes in order to meet some pre-defined system performance objectives.
Medium term scheduling is essentially concerned with memory management, hence it's very often designed as a part of the memory management subsystem of an OS. Its efficient interaction with the short term scheduler is essential for system performances, especially in virtual memory systems. This is the reason why in paged system the pager process is usually run at a very high (dispatching) priority level. 
Long term scheduling obviously controls the degree of multiprogramming in multitasking systems, following certain policies to decide whether the system can honour a new job submission or, if more than one job is submitted, which of them should be selected. The need for some form of compromise between degree of multiprogramming and throughput seems evident, especially when one considers interactive systems.

    23. Describe with the aid of a diagram the life-cycle of a process. You should describe each of the states that it can be in, and the reasons why it moves between these states. 



As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running.
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    24. What information does the operating system keep in the process control block?

Information associated with each process.
    • Process state
    • Program counter
    • CPU registers
    • CPU scheduling information
    • Memory-management information
    • Accounting information
    • I/O status information

    25. Use an example to explain the need for process synchronization.

Cooperating processes may either share a logical address space (ie. Both code and data), or be allowed to share data through files. Process synchronization is, therefore, needed to prevent data inconsistencies where concurrent access by two processes on shared data may occur.

    26. What are critical section and race conditions?

    • Critical section: A section of code for which mutual exclusion must be implemented

    • Race condition: The situation where several processes access – and manipulate shared data concurrently. The final value of the shared data depends upon which process finishes last.

    27. Describe the purpose of semaphores and how they can be used to solve synchronization problems.

Semaphores are nonnegative integer variables that are used as a flag in and OS. The semaphore signals if and when a resource is free and can be used by a process

Semaphores are synchronization tools that can be used to solve synchronization problems by using the atomic operations: wait (p) and signal (v). For instance if two concurrent processes P1 and P2 with statements S1 and S2 respectively require that S2 be executed after S1 has completed then the scheme can be implemented by allowing P1 and P2 to share a common semaphore synch, initialized to 0, and by inserting statements in P1 to allow waiting and signaling and statements in P2 to wait, the synchronization problem can be solved.


    28. How can semaphores be used to protect critical sections? Which programming errors can occur?

A semaphore is assigned to the resource, it's shared among all processes that need to access the resource, and its counter is initialized to 1. A process then waits on the semaphore upon entering the critical section for the resource, and signals on leaving it. The first process will get access. If another process arrives while the former is still in the critical section, it'll be blocked, and so will further processes. In this situation the absolute value of the counter is equal to the number of processes waiting to enter the critical section. Every process leaving the critical section will let a waiting process use the resource by signaling the semaphore.

Programming errors
    • Starvation
    • Deadlock

    29. How can a message system be used to achieve synchronization?
Messages are yet another form of shared resource, through which cooperating processes can communicate data to each other. intuitively, we can think of implementing them by means of two primitives like 
    • send(dest, msg): to send a message to a designated process, destination of the message; 
    • receive(source, msg): to receive a message from a designated process; 
Synchronization: the primitives can cause the destination or source of the message to block until the delivery is completed (successfully or not). In this case they are called blocking, and nonblocking otherwise. The reception can also be either explicitly confirmed through the use of the analog of a return receipt or not. 
    30. Explain the term deadlock.

A set of blocked processes each holding a resource and waiting to acquire a resource held by another process in the set.

    31. What are resource allocation graphs?
A set of vertices V and a set of edges E.
    • V is partitioned into two types:
        ◦ P = {P1, P2, …,Pn}, the set consisting of all the processes in the system.
        ◦ R = {R1, R2, …,Rm}, the set consisting of all resource types in the system.
    • request edge – directed edge P1 ®Rj
    • assignment edge – directed edge Rj ®Pi

    32. Name the necessary conditions for a deadlock.

Deadlock can arise if four conditions hold simultaneously.

    • Mutual exclusion:  only one process at a time can use a resource.
    • Hold and wait:  a process holding at least one resource is waiting to acquire additional resources held by other processes.
    • No preemption:  a resource can be released only voluntarily by the process holding it, after that process has completed its task.
    • Circular wait:  there exists a set {P0, P1, …, P0} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for a resource that is held by P2, …, Pn–1 is waiting for a resource that is held by 
Pn, and P0 is waiting for a resource that is held by P0.

    33. Can deadlocks be prevented?

Yes, deadlocks can be prevented when you ensure that at least one of the four conditions cannot hold

    • Mutual Exclusion – not required for sharable resources; must hold for nonsharable resources.
    • Hold and Wait – must guarantee that whenever a process requests a resource, it does not hold any other resources.
        ◦ Require process to request and be allocated all its resources before it begins execution, or allow process to request resources only when the process has none.
        ◦ Low resource utilization; starvation possible.
    • No Preemption –
        ◦ If a process that is holding some resources requests another resource that cannot be immediately allocated to it, then all resources currently being held are released.
        ◦ Preempted resources are added to the list of resources for which the process is waiting.
        ◦ Process will be restarted only when it can regain its old resources, as well as the new ones that it is requesting.
    • Circular Wait – impose a total ordering of all resource types, and require that each process requests resources in an increasing order of enumeration.

    34. Is it possible to detect deadlocks? If yes, how can detected deadlocks be resolved?

Yes, deadlocks can be detected using a detection algorithm (Periodically invoke an algorithm that searches for the deadlock), if the resource allocation graphs contains no cycles, then there are no deadlocks, if a graph contains a cycle: i.e. if only one instance per resource type, then there is a deadlock, if several instances per resource type there is a possibility of a deadlock
	
Deadlocks can be resolved in the following ways
    • Abort all deadlocked processes
    • Abort one process at a time until the deadlock cycle is eliminated.

    3. Memory

    35. Describe the following memory allocation algorithms: 
        a. First fit: Allocate the first hole that is big enough. 
        b. Best fit: Allocate the smallest hole that is big enough; must search entire list, unless ordered by size. Produces the smallest leftover hole.
        c. Worst fit: Allocate the largest hole; must also search entire list. Produces the largest leftover hole.
    36. Explain the term “swapping”.

Swapping is a mechanism of removing a process temporarily out of memory to a backing store and then, and then brought back into memory for continued execution.

Example: Assume a multiprogramming environment with a round robin CPU scheduling algorithm. When a quantum expires, the memory manager will start to swap out the process that just finished, and swap in another process to the memory space that has just been freed.


    37. What is the difference between internal and external fragmentation?

    • Internal Fragmentation – allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used
    • External Fragmentation – total memory space exists to satisfy a request, but it is not contiguous; storage is fragmented into a large number of small holes

    38. Explain the term “segmentation”.

    • Memory-management scheme that supports user view of memory. 

    39. What is compaction and in which context is it used?

Compaction is referred to as garbage collection, performed by the OS to reclaim fragmented sections of the memory space. 



Its should be used when;
    • When a certain percentage of memory becomes busy, say 80%.  
    • When there are processes waiting to get in. 
    • After a prescribed amount of time has elapsed

    40. Explain the term “paging”.

Paging is a memory management scheme that permits the physical address space of a process to be noncontiguous.???

    41. What are page faults? How are they handled by an operating system?

Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 



    42. Explain the term “virtual memory”.

A technique that allows programs to be executed even though they are not stored entirely in memory. It gives a user the illusion that a large amount of main memory is available when, in fact, it is not. 

    43. Name three loading and three replacement strategies and discuss their advantages and disadvantages.













    44. Why are segmentation and paging sometimes combined into one scheme? 

To remove the problems of external fragmentation and compaction, because the pages are of fixed length

    45. In the context of memory management, under which circumstances do external and internal fragmentations occur? How can each be handled? 

Internal Fragmentation occurs when fixed partitions suffer from inefficient memory use - any process, no matter how small, occupies an entire partition.

Internal fragmentation can be solved by combining free areas of memory whenever possible and combining it to form a block that will be deallocate.

External Fragmentation occurs when process are loaded and removed from memory, the free memory space is broken down into little pieces. 

External fragmentation can be handle by compaction; the OS reclaims fragmented sections of the memory space and compacts them to make one large block of memory that is large enough to accommodate some or all of the jobs waiting to get in. 


    46. Under what circumstances do page faults occur? Describe the actions taken by the operating system when a page fault occurs. 
Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 

    47. When virtual memory is implemented in a computing system, it carries certain costs and certain benefits. List those costs and the benefits. It is possible for the costs to exceed the benefits. Explain what measures you can take to ensure that this imbalance does not occur.

Benefits
    • Users can write extremely very large programs without being concerned with the limitations of main memory. 
    • Increases multiprogramming with a corresponding increase in CPU utilization and throughput, but with no increase in response time or turnaround time. 
    • Less I/O would be needed to load or swap each user program into memory, so each user program would run faster. 

    48. What is the cause of thrashing? How does the system detect thrashing? Once it detects thrashing, what can the system do to eliminate this problem? 

Thrashing is caused when a process is busy swapping pages in and out. The process spend more time paging than executing

Thrashing can be detected by;
    • Low CPU utilization

Thrashing can be limited by using a local replacement algorithm; if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash also.

    49. What is meant by the term demand paging in a virtual memory management system, and how is it implemented?

Demand paging is swapping a process into memory when it is needed

Implementation
    • Page is needed Þ reference to it
    • invalid reference Þ abort
    • not-in-memory Þ bring to memory
    1. Introduction 
    1. Describe the main purpose of an operating system.

The main purpose of an OS is to act as an intermediary between the user of a computer and the computer hardware by providing an environment in which a user can execute programs in a convenient and efficient manner.

    2. Identify 3 activities of an operating system in respect to process management, memory management, I/O management and file management. 
	Process Management:
        ◦ Process creation and deletion.
        ◦ Process suspension and resumption.
        ◦ Provision of mechanisms for:
            i. process synchronization
            ii. process communication
	Memory Management:
                • Keep track of which parts of memory are currently being used and by whom.
                • Decide which processes to load when memory space becomes available.
                • Allocate and deallocate memory space as needed.
	File Management:
    • File creation and deletion.
    • Directory creation and deletion.
    • Support of primitives for manipulating files and directories.
    • Mapping files onto secondary storage.
    • File backup on stable (nonvolatile) storage media.
	I/O Management:
        ◦ A buffer-caching system 
        ◦ A general device-driver interface
        ◦ Drivers for specific hardware devices
	Secondary Storage Management:
    • Free space management
    • Storage allocation
    • Disk scheduling
	
    3. Name two storage media with random access and two media with sequential access.
	Random Access:
    • RAM
    • Registers
    • Cache
	Sequential Access:
    • Magnetic Disks???
    • Optical Disks
    • Magnetic Tapes

    4. What are the differences between a trap and an interrupt? What is the use of each function? 

	A trap is a software-generated interrupt caused either by an error or a specific 	user program request.
An Interrupt is an asynchronous break in program flow that occurs as a result of events outside the running program. It’s usually hardware related, stemming from events such as a button press, timer expiration, or completion of a data transfer. An Interrupt triggers execution of instructions that perform work on behalf of the system, but not necessarily the current program. They were developed for exception handling and were later applied to I/O events.
    5. What are the purposes of system calls and system programs?

System calls provide an interface between the process and the operating system. System calls allow user-level processes to request some services from the operating system which process itself is not allowed to do. For example, for I/O a process involves a system call telling the operating system to read or write particular area and this request is satisfied by the operating system.
	
The purpose of a system program is to provide a convenient environment for   program development and execution.  


    6. What are the main advantages of multiprogramming?

    • Increases CPU utilization by organizing  jobs so that the CPU always has one to execute
    • Allows time sharing: many users can use a computer system interactively at the same time

	
    7. What is multi tasking, multi programming, multi threading? 

Multi tasking: The ability to execute more than one task at the same time

Multi programming: The ability to run several programs on a uniprocessor.

Multi threading:The ability of an operating system to execute different parts of a program, called threads, simultaneously. ***??

    8. What is a Real-Time System? 

A real-time system is one with well defined, fixed time constraints whereby processing must be done within the defined constraints or the system will fail


    9. What is the difference between Hard and Soft real-time systems? What is a mission critical system?
    • Hard real-time
        ◦ Secondary storage limited or absent, data stored in short term memory, or read-only memory (ROM)
        ◦ Conflicts with time-sharing systems, not supported by general-purpose operating systems.
        ◦ Guarantees that all critical tasks be completed on time
    • Soft real-time
        ◦ Limited utility in industrial control of robotics
        ◦ Useful in applications (multimedia, virtual reality) requiring advanced operating-system features.
        ◦ A critical real-time task gets priority over other tasks and retains that priority until it completes

    • Mission Critical System: 
	


    2. Processes

    10. Explain the difference between a program and a process.

A process is a program loaded into memory and executing. It is a unit of work in a system. 

A program is lines of code or algorithm expressed in some suitable notation.

    11. What are typical process states and typical transitions between these states.

	As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • Waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.



 


Transitions between the states

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → Waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out  (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running. 
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    12. Why are program counter and processor registers part of the process control block?
The process control block represents each process in the O/S. 

    • The program counter contains the address of the next instruction for execution for 	this process
    • The processor registers are a volatile storage medium that are used to store the current state of a process when an interrupt occurs. They allow the process to be 	continued correctly afterwards 

    13. Explain the main differences between processes and threads.
Differences
    • Unlike processes, threads are not independent of one another. 
    • Unlike processes, all threads can access every address in the task. 
    • Unlike processes, threads are designed to assist one other. Note that processes might or might not assist one another because processes may originate from different users. 
	Similarities
    • Like processes threads share CPU and only one thread active (running) at a time. 
    • Like processes, threads within a process, execute sequentially. 
    • Like processes, threads can create children. 
    • And like process, if one thread is blocked, another thread can run. 

    14. What are the requirements for a good scheduler?

    • Max CPU utilization – keep the CPU as busy as possible
    • Max Throughput – Number of processes that complete their execution per time unit
    • Min Turnaround time – amount of time to execute a particular process
    • Min Waiting time – amount of time a process has been waiting in the ready queue
    • Min Response time – amount of time it takes from when a request was submitted until the first response is produced, not output  (for time-sharing environment)

    15. Explain the difference between preemptive and non-preemptive schedulers.

    • Nonpreemptive – once CPU given to the process it cannot be interrupted until it completes its CPU burst or switches to a waiting state.
    • Preemptive – if a new process arrives with CPU burst length less than remaining time of current executing process, preempt.  This scheme is known as the Shortest-Remaining-Time-First (SRTF).

Explain why strict nonpreemptive scheduling is unlikely to be used in a computer center.
	Convoy effect, short jobs are made to wait by longer jobs

    16. Explain the scheduling strategies FCFS, LPTF, SJF, SRTF, and RR.

    • FCFS: (First Come First Served) process that requests the CPU first, gets the CPU first (FIFO)

    • SJF: (Shortest Job First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the smallest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    • SRTF: (Shortest Remaining Time First) pre-emptive SJF scheduling: When a process with a shorter next CPU burst than what is left of the currently executing process arrives in the ready queue, it will preempt the currently executing process

    • RR: (Round Robin scheduling) designed especially for timesharing systems. Similar to FCFS but preemption is added to switch between processes. A small unit called a time quantum or time slice is defined. The ready queue is treated as a circular queue. The CPU scheduler goes around the ready queue allocating the CPU for each process for a time interval of up to one time quantum


    • LPTF: (Long Processing Time First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the longest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    17. What are two differences between user-level threads and kernel-level threads? Under what circumstances is one type better than the another?

    • User-level threads are faster to create and require less overhead than kernel-level threads.
    • The kernel knows about kernel-level threads, but not user-level threads, so K/L threads can be scheduled by the kernel’s scheduler.

K/L threads are better than U/L threads when you will have threads that block, for example, on I/O. In these cases, the scheduler can choose another K/L to run so that the whole process doesn’t block.

U/L threads are better when there will be a lot of them created, or when they will be created and destroyed relatively quickly because there is little creation/destruction overhead. They are also better when the threads need to communicate because thread communication is simply a procedure call in U/L threads, which is faster than going through the kernel.

Short Answer
    • User threads are built using a library and are unknown or undifferentiated by the kernel. 
    • Kernel threads require more operating system resources because they are managed by the kernel. 
A user thread is more appropriate for low-level tasks, whereas a kernel thread is better for high-priority tasks that should get high priority to system resources

    18. What resources are used when a thread is created? How do they differ from those used when a process is created?

When a thread is created, it needs its own execution state. This basically involves allocating it memory for its stack, and giving it a PC, SP, and storage for its other registers, all within the processes address space.

When a process is created, it receives an entirely new address space, which means that the system has to create a new mapping from addresses to physical memory, and it has to allocate the process a new block of physical memory. Within that address space, the system must keep track of the open files, the process’s permissions, etc. It must also allocate things that are required for each thread within that process to have its own execution state, including a place in memory to store the stack and the registers for each thread in the process, as listed above.

So the resources used in creating a thread are a strict (and much smaller) subset of those used when creating a process.

Short Answer
When created, a thread requires an ID, a register set, a stack and program counter. 
Threads are smaller than processes. In addition to the thread information, each process creates its own Process Control Block (PCB), containing a code section, data section, and O/S resources. Threads reference these items from their creating or parent process.

    19. Consider the following set of processes, with the length of the CPU-burst time given in milliseconds: 
Process
Burst Time 
Priority
P1
10
3
P2
1
1
P3
2
3
P4
1
4
P5
5
2
The processes are assumed to have arrived in the order PI, P2, P3, P4, P5, all at time 0 
        a. Draw four Gantt charts that illustrate the execution of these processes using FCFS, SJF, a nonpreemptive priority (a smaller priority number implies a higher priority), and RR (quantum = 1) scheduling.
FCFS
P1
P2
P3
P4
P5
	          0			           10     11              13     14                             19
		      SJF	
P2
P4
P3
P5
P1
		           0      1	     2                4                               9                                              19


No preemptive Priority
P2
P5
P1
P3
P4
     0      1                             6                                       16                    18    19
Round Robin
P1
P2
P3
P4
P5
P1
P3
P5
P1
P5
P1
P5
P1
P5
P1
P1
P1
P1
P1
0     1      2     3      4      5      6      7      8      9     10   11   12     13   14     15    16   17    18      19

        b. What is the turnaround time of each process for each of the scheduling algorithms in part a?
Turnaround time = Completion time – Arrival time 
FCFS
P1, 10 – 0 = 10, P2, 11 – 0 = 11, P3, 13 – 0 = 13, P4, 14 – 0 = 14, P5, 19 – 0 = 19
SJF 
P1, 19 – 0 = 19, P2, 1 – 0 = 1, P3, 4 – 0 = 4, P4, 2 – 0 = 2, P5, 9 – 0 = 9
No preemptive Priority
P1, 16 – 0 = 16, P2, 1 – 0 = 1, P3, 18 – 0 = 18, P4, 19 – 0 = 19, P5, 6 – 0 = 6
Round Robin
P1, 19 – 0 = 19, P2, 2 – 0 = 2, P3, 7 – 0 = 7, P4, 4 – 0 = 4, P5, 14 – 0 = 14
        c. What is the waiting time of each process for each of the scheduling algorithms in part a?
Waiting time = Turnaround time – Burst time
FCFS
P1, 10 – 10 = 0, P2, 11 – 1 = 10, P3, 13 – 2 = 11, P4, 14 – 1 = 13, P5, 19 – 5 = 14
SJF 
P1, 19 – 10 = 9, P2, 1 – 1 = 0, P3, 4 – 2 = 2, P4, 2 – 1 = 1, P5, 9 – 5 = 4
No preemptive Priority
P1, 16 – 10 = 6, P2, 1 – 1 = 0, P3, 18 – 2 = 16, P4, 19 – 1 = 18, P5, 6 – 5 = 1
Round Robin
P1, 19 – 10 = 9, P2, 2 – 1 = 1, P3, 7 – 2 = 5, P4, 4 – 1 = 3, P5, 14 – 5 = 9
        d. Which of the schedules in part a result in the minimal average waiting time (over all processes)?
FCFS: 0+10+11+13+14 = 48/5 = 9.6
SJF: 9+0+2+1+4 = 16/5 = 3.2
No preemptive Priority: 6+0+16+18+1 = 41/5 = 8.2
Round Robin: 9+1+5+3+9 = 27/5 = 5.4
The shortest average waiting time is Shortest Job First (SJF) at an average of 3.2 milliseconds
    20. What advantage is there in having different time-quantum sizes on different levels of a multilevel queuing system? 

Because each queue must have more priority over lower-priority queues in a multi level queuing system, the different time-quantum sizes help in distinguishing between the queues by priority and determining the CPU time slices each queue will have. For instance

    21. Many CPU scheduling algorithms are parameterized. For example, the RR algorithm requires a parameter to indicate the time slice. Multilevel feedback queues require parameters to define the number of queues, the scheduling algorithms for each queue, the criteria used to move processes between queues, and so on. These algorithms are thus really sets of algorithms (for example, the set of RR algorithms for all time slices, and so on). One set of algorithms may include another (for example, the FCFS algorithm is the RR algorithm with an infinite time quantum). What (if any) relation holds between the following pairs of sets of algorithms? 
        a. Priority and SJF 
The SJF algorithm is a special case of the general priority-scheduling algorithm. A priority is associated with each process, and the CPU is allocated to a process with the highest priority. Equal priority processes are scheduled in FCFS order.
An SJF algorithm is simply a priority algorithm where the priority p is simply the inverse of the predicted next CPU burst time. The larger the burst time the lower the priority, and vice versa.
        b. Multilevel feedback queues and FCFS 
Processes in the lowest priority queues are scheduled in a FCFS order
        c. Priority and FCFS 
Equal priority processes are scheduled in a FCFS order.
        d. RR and SJF 
No relationship
    22. Describe the differences among short-term, medium-term, and long-term scheduling.

Short Answer
    • Short-term scheduling - also called "CPU scheduler", allocates CPU time to processes in memory 
    • Medium-term scheduling - swaps processes out of memory and reinstates them, to be run, at a later time, as in a time-sharing system 
    • Long-term scheduling - also called "job scheduler" 
A common difference, other than their defined tasks, is also how frequently each scheduler is called.
Long Answer
Short term scheduling concerns with the allocation of CPU time to processes in order to meet some pre-defined system performance objectives.
Medium term scheduling is essentially concerned with memory management, hence it's very often designed as a part of the memory management subsystem of an OS. Its efficient interaction with the short term scheduler is essential for system performances, especially in virtual memory systems. This is the reason why in paged system the pager process is usually run at a very high (dispatching) priority level. 
Long term scheduling obviously controls the degree of multiprogramming in multitasking systems, following certain policies to decide whether the system can honour a new job submission or, if more than one job is submitted, which of them should be selected. The need for some form of compromise between degree of multiprogramming and throughput seems evident, especially when one considers interactive systems.

    23. Describe with the aid of a diagram the life-cycle of a process. You should describe each of the states that it can be in, and the reasons why it moves between these states. 



As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running.
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    24. What information does the operating system keep in the process control block?

Information associated with each process.
    • Process state
    • Program counter
    • CPU registers
    • CPU scheduling information
    • Memory-management information
    • Accounting information
    • I/O status information

    25. Use an example to explain the need for process synchronization.

Cooperating processes may either share a logical address space (ie. Both code and data), or be allowed to share data through files. Process synchronization is, therefore, needed to prevent data inconsistencies where concurrent access by two processes on shared data may occur.

    26. What are critical section and race conditions?

    • Critical section: A section of code for which mutual exclusion must be implemented

    • Race condition: The situation where several processes access – and manipulate shared data concurrently. The final value of the shared data depends upon which process finishes last.

    27. Describe the purpose of semaphores and how they can be used to solve synchronization problems.

Semaphores are nonnegative integer variables that are used as a flag in and OS. The semaphore signals if and when a resource is free and can be used by a process

Semaphores are synchronization tools that can be used to solve synchronization problems by using the atomic operations: wait (p) and signal (v). For instance if two concurrent processes P1 and P2 with statements S1 and S2 respectively require that S2 be executed after S1 has completed then the scheme can be implemented by allowing P1 and P2 to share a common semaphore synch, initialized to 0, and by inserting statements in P1 to allow waiting and signaling and statements in P2 to wait, the synchronization problem can be solved.


    28. How can semaphores be used to protect critical sections? Which programming errors can occur?

A semaphore is assigned to the resource, it's shared among all processes that need to access the resource, and its counter is initialized to 1. A process then waits on the semaphore upon entering the critical section for the resource, and signals on leaving it. The first process will get access. If another process arrives while the former is still in the critical section, it'll be blocked, and so will further processes. In this situation the absolute value of the counter is equal to the number of processes waiting to enter the critical section. Every process leaving the critical section will let a waiting process use the resource by signaling the semaphore.

Programming errors
    • Starvation
    • Deadlock

    29. How can a message system be used to achieve synchronization?
Messages are yet another form of shared resource, through which cooperating processes can communicate data to each other. intuitively, we can think of implementing them by means of two primitives like 
    • send(dest, msg): to send a message to a designated process, destination of the message; 
    • receive(source, msg): to receive a message from a designated process; 
Synchronization: the primitives can cause the destination or source of the message to block until the delivery is completed (successfully or not). In this case they are called blocking, and nonblocking otherwise. The reception can also be either explicitly confirmed through the use of the analog of a return receipt or not. 
    30. Explain the term deadlock.

A set of blocked processes each holding a resource and waiting to acquire a resource held by another process in the set.

    31. What are resource allocation graphs?
A set of vertices V and a set of edges E.
    • V is partitioned into two types:
        ◦ P = {P1, P2, …,Pn}, the set consisting of all the processes in the system.
        ◦ R = {R1, R2, …,Rm}, the set consisting of all resource types in the system.
    • request edge – directed edge P1 ®Rj
    • assignment edge – directed edge Rj ®Pi

    32. Name the necessary conditions for a deadlock.

Deadlock can arise if four conditions hold simultaneously.

    • Mutual exclusion:  only one process at a time can use a resource.
    • Hold and wait:  a process holding at least one resource is waiting to acquire additional resources held by other processes.
    • No preemption:  a resource can be released only voluntarily by the process holding it, after that process has completed its task.
    • Circular wait:  there exists a set {P0, P1, …, P0} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for a resource that is held by P2, …, Pn–1 is waiting for a resource that is held by 
Pn, and P0 is waiting for a resource that is held by P0.

    33. Can deadlocks be prevented?

Yes, deadlocks can be prevented when you ensure that at least one of the four conditions cannot hold

    • Mutual Exclusion – not required for sharable resources; must hold for nonsharable resources.
    • Hold and Wait – must guarantee that whenever a process requests a resource, it does not hold any other resources.
        ◦ Require process to request and be allocated all its resources before it begins execution, or allow process to request resources only when the process has none.
        ◦ Low resource utilization; starvation possible.
    • No Preemption –
        ◦ If a process that is holding some resources requests another resource that cannot be immediately allocated to it, then all resources currently being held are released.
        ◦ Preempted resources are added to the list of resources for which the process is waiting.
        ◦ Process will be restarted only when it can regain its old resources, as well as the new ones that it is requesting.
    • Circular Wait – impose a total ordering of all resource types, and require that each process requests resources in an increasing order of enumeration.

    34. Is it possible to detect deadlocks? If yes, how can detected deadlocks be resolved?

Yes, deadlocks can be detected using a detection algorithm (Periodically invoke an algorithm that searches for the deadlock), if the resource allocation graphs contains no cycles, then there are no deadlocks, if a graph contains a cycle: i.e. if only one instance per resource type, then there is a deadlock, if several instances per resource type there is a possibility of a deadlock
	
Deadlocks can be resolved in the following ways
    • Abort all deadlocked processes
    • Abort one process at a time until the deadlock cycle is eliminated.

    3. Memory

    35. Describe the following memory allocation algorithms: 
        a. First fit: Allocate the first hole that is big enough. 
        b. Best fit: Allocate the smallest hole that is big enough; must search entire list, unless ordered by size. Produces the smallest leftover hole.
        c. Worst fit: Allocate the largest hole; must also search entire list. Produces the largest leftover hole.
    36. Explain the term “swapping”.

Swapping is a mechanism of removing a process temporarily out of memory to a backing store and then, and then brought back into memory for continued execution.

Example: Assume a multiprogramming environment with a round robin CPU scheduling algorithm. When a quantum expires, the memory manager will start to swap out the process that just finished, and swap in another process to the memory space that has just been freed.


    37. What is the difference between internal and external fragmentation?

    • Internal Fragmentation – allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used
    • External Fragmentation – total memory space exists to satisfy a request, but it is not contiguous; storage is fragmented into a large number of small holes

    38. Explain the term “segmentation”.

    • Memory-management scheme that supports user view of memory. 

    39. What is compaction and in which context is it used?

Compaction is referred to as garbage collection, performed by the OS to reclaim fragmented sections of the memory space. 



Its should be used when;
    • When a certain percentage of memory becomes busy, say 80%.  
    • When there are processes waiting to get in. 
    • After a prescribed amount of time has elapsed

    40. Explain the term “paging”.

Paging is a memory management scheme that permits the physical address space of a process to be noncontiguous.???

    41. What are page faults? How are they handled by an operating system?

Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 



    42. Explain the term “virtual memory”.

A technique that allows programs to be executed even though they are not stored entirely in memory. It gives a user the illusion that a large amount of main memory is available when, in fact, it is not. 

    43. Name three loading and three replacement strategies and discuss their advantages and disadvantages.













    44. Why are segmentation and paging sometimes combined into one scheme? 

To remove the problems of external fragmentation and compaction, because the pages are of fixed length

    45. In the context of memory management, under which circumstances do external and internal fragmentations occur? How can each be handled? 

Internal Fragmentation occurs when fixed partitions suffer from inefficient memory use - any process, no matter how small, occupies an entire partition.

Internal fragmentation can be solved by combining free areas of memory whenever possible and combining it to form a block that will be deallocate.

External Fragmentation occurs when process are loaded and removed from memory, the free memory space is broken down into little pieces. 

External fragmentation can be handle by compaction; the OS reclaims fragmented sections of the memory space and compacts them to make one large block of memory that is large enough to accommodate some or all of the jobs waiting to get in. 


    46. Under what circumstances do page faults occur? Describe the actions taken by the operating system when a page fault occurs. 
Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 

    47. When virtual memory is implemented in a computing system, it carries certain costs and certain benefits. List those costs and the benefits. It is possible for the costs to exceed the benefits. Explain what measures you can take to ensure that this imbalance does not occur.

Benefits
    • Users can write extremely very large programs without being concerned with the limitations of main memory. 
    • Increases multiprogramming with a corresponding increase in CPU utilization and throughput, but with no increase in response time or turnaround time. 
    • Less I/O would be needed to load or swap each user program into memory, so each user program would run faster. 

    48. What is the cause of thrashing? How does the system detect thrashing? Once it detects thrashing, what can the system do to eliminate this problem? 

Thrashing is caused when a process is busy swapping pages in and out. The process spend more time paging than executing

Thrashing can be detected by;
    • Low CPU utilization

Thrashing can be limited by using a local replacement algorithm; if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash also.

    49. What is meant by the term demand paging in a virtual memory management system, and how is it implemented?

Demand paging is swapping a process into memory when it is needed

Implementation
    • Page is needed Þ reference to it
    • invalid reference Þ abort
    • not-in-memory Þ bring to memory
    1. Introduction 
    1. Describe the main purpose of an operating system.

The main purpose of an OS is to act as an intermediary between the user of a computer and the computer hardware by providing an environment in which a user can execute programs in a convenient and efficient manner.

    2. Identify 3 activities of an operating system in respect to process management, memory management, I/O management and file management. 
	Process Management:
        ◦ Process creation and deletion.
        ◦ Process suspension and resumption.
        ◦ Provision of mechanisms for:
            i. process synchronization
            ii. process communication
	Memory Management:
                • Keep track of which parts of memory are currently being used and by whom.
                • Decide which processes to load when memory space becomes available.
                • Allocate and deallocate memory space as needed.
	File Management:
    • File creation and deletion.
    • Directory creation and deletion.
    • Support of primitives for manipulating files and directories.
    • Mapping files onto secondary storage.
    • File backup on stable (nonvolatile) storage media.
	I/O Management:
        ◦ A buffer-caching system 
        ◦ A general device-driver interface
        ◦ Drivers for specific hardware devices
	Secondary Storage Management:
    • Free space management
    • Storage allocation
    • Disk scheduling
	
    3. Name two storage media with random access and two media with sequential access.
	Random Access:
    • RAM
    • Registers
    • Cache
	Sequential Access:
    • Magnetic Disks???
    • Optical Disks
    • Magnetic Tapes

    4. What are the differences between a trap and an interrupt? What is the use of each function? 

	A trap is a software-generated interrupt caused either by an error or a specific 	user program request.
An Interrupt is an asynchronous break in program flow that occurs as a result of events outside the running program. It’s usually hardware related, stemming from events such as a button press, timer expiration, or completion of a data transfer. An Interrupt triggers execution of instructions that perform work on behalf of the system, but not necessarily the current program. They were developed for exception handling and were later applied to I/O events.
    5. What are the purposes of system calls and system programs?

System calls provide an interface between the process and the operating system. System calls allow user-level processes to request some services from the operating system which process itself is not allowed to do. For example, for I/O a process involves a system call telling the operating system to read or write particular area and this request is satisfied by the operating system.
	
The purpose of a system program is to provide a convenient environment for   program development and execution.  


    6. What are the main advantages of multiprogramming?

    • Increases CPU utilization by organizing  jobs so that the CPU always has one to execute
    • Allows time sharing: many users can use a computer system interactively at the same time

	
    7. What is multi tasking, multi programming, multi threading? 

Multi tasking: The ability to execute more than one task at the same time

Multi programming: The ability to run several programs on a uniprocessor.

Multi threading:The ability of an operating system to execute different parts of a program, called threads, simultaneously. ***??

    8. What is a Real-Time System? 

A real-time system is one with well defined, fixed time constraints whereby processing must be done within the defined constraints or the system will fail


    9. What is the difference between Hard and Soft real-time systems? What is a mission critical system?
    • Hard real-time
        ◦ Secondary storage limited or absent, data stored in short term memory, or read-only memory (ROM)
        ◦ Conflicts with time-sharing systems, not supported by general-purpose operating systems.
        ◦ Guarantees that all critical tasks be completed on time
    • Soft real-time
        ◦ Limited utility in industrial control of robotics
        ◦ Useful in applications (multimedia, virtual reality) requiring advanced operating-system features.
        ◦ A critical real-time task gets priority over other tasks and retains that priority until it completes

    • Mission Critical System: 
	


    2. Processes

    10. Explain the difference between a program and a process.

A process is a program loaded into memory and executing. It is a unit of work in a system. 

A program is lines of code or algorithm expressed in some suitable notation.

    11. What are typical process states and typical transitions between these states.

	As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • Waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.



 


Transitions between the states

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → Waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out  (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running. 
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    12. Why are program counter and processor registers part of the process control block?
The process control block represents each process in the O/S. 

    • The program counter contains the address of the next instruction for execution for 	this process
    • The processor registers are a volatile storage medium that are used to store the current state of a process when an interrupt occurs. They allow the process to be 	continued correctly afterwards 

    13. Explain the main differences between processes and threads.
Differences
    • Unlike processes, threads are not independent of one another. 
    • Unlike processes, all threads can access every address in the task. 
    • Unlike processes, threads are designed to assist one other. Note that processes might or might not assist one another because processes may originate from different users. 
	Similarities
    • Like processes threads share CPU and only one thread active (running) at a time. 
    • Like processes, threads within a process, execute sequentially. 
    • Like processes, threads can create children. 
    • And like process, if one thread is blocked, another thread can run. 

    14. What are the requirements for a good scheduler?

    • Max CPU utilization – keep the CPU as busy as possible
    • Max Throughput – Number of processes that complete their execution per time unit
    • Min Turnaround time – amount of time to execute a particular process
    • Min Waiting time – amount of time a process has been waiting in the ready queue
    • Min Response time – amount of time it takes from when a request was submitted until the first response is produced, not output  (for time-sharing environment)

    15. Explain the difference between preemptive and non-preemptive schedulers.

    • Nonpreemptive – once CPU given to the process it cannot be interrupted until it completes its CPU burst or switches to a waiting state.
    • Preemptive – if a new process arrives with CPU burst length less than remaining time of current executing process, preempt.  This scheme is known as the Shortest-Remaining-Time-First (SRTF).

Explain why strict nonpreemptive scheduling is unlikely to be used in a computer center.
	Convoy effect, short jobs are made to wait by longer jobs

    16. Explain the scheduling strategies FCFS, LPTF, SJF, SRTF, and RR.

    • FCFS: (First Come First Served) process that requests the CPU first, gets the CPU first (FIFO)

    • SJF: (Shortest Job First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the smallest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    • SRTF: (Shortest Remaining Time First) pre-emptive SJF scheduling: When a process with a shorter next CPU burst than what is left of the currently executing process arrives in the ready queue, it will preempt the currently executing process

    • RR: (Round Robin scheduling) designed especially for timesharing systems. Similar to FCFS but preemption is added to switch between processes. A small unit called a time quantum or time slice is defined. The ready queue is treated as a circular queue. The CPU scheduler goes around the ready queue allocating the CPU for each process for a time interval of up to one time quantum


    • LPTF: (Long Processing Time First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the longest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    17. What are two differences between user-level threads and kernel-level threads? Under what circumstances is one type better than the another?

    • User-level threads are faster to create and require less overhead than kernel-level threads.
    • The kernel knows about kernel-level threads, but not user-level threads, so K/L threads can be scheduled by the kernel’s scheduler.

K/L threads are better than U/L threads when you will have threads that block, for example, on I/O. In these cases, the scheduler can choose another K/L to run so that the whole process doesn’t block.

U/L threads are better when there will be a lot of them created, or when they will be created and destroyed relatively quickly because there is little creation/destruction overhead. They are also better when the threads need to communicate because thread communication is simply a procedure call in U/L threads, which is faster than going through the kernel.

Short Answer
    • User threads are built using a library and are unknown or undifferentiated by the kernel. 
    • Kernel threads require more operating system resources because they are managed by the kernel. 
A user thread is more appropriate for low-level tasks, whereas a kernel thread is better for high-priority tasks that should get high priority to system resources

    18. What resources are used when a thread is created? How do they differ from those used when a process is created?

When a thread is created, it needs its own execution state. This basically involves allocating it memory for its stack, and giving it a PC, SP, and storage for its other registers, all within the processes address space.

When a process is created, it receives an entirely new address space, which means that the system has to create a new mapping from addresses to physical memory, and it has to allocate the process a new block of physical memory. Within that address space, the system must keep track of the open files, the process’s permissions, etc. It must also allocate things that are required for each thread within that process to have its own execution state, including a place in memory to store the stack and the registers for each thread in the process, as listed above.

So the resources used in creating a thread are a strict (and much smaller) subset of those used when creating a process.

Short Answer
When created, a thread requires an ID, a register set, a stack and program counter. 
Threads are smaller than processes. In addition to the thread information, each process creates its own Process Control Block (PCB), containing a code section, data section, and O/S resources. Threads reference these items from their creating or parent process.

    19. Consider the following set of processes, with the length of the CPU-burst time given in milliseconds: 
Process
Burst Time 
Priority
P1
10
3
P2
1
1
P3
2
3
P4
1
4
P5
5
2
The processes are assumed to have arrived in the order PI, P2, P3, P4, P5, all at time 0 
        a. Draw four Gantt charts that illustrate the execution of these processes using FCFS, SJF, a nonpreemptive priority (a smaller priority number implies a higher priority), and RR (quantum = 1) scheduling.
FCFS
P1
P2
P3
P4
P5
	          0			           10     11              13     14                             19
		      SJF	
P2
P4
P3
P5
P1
		           0      1	     2                4                               9                                              19


No preemptive Priority
P2
P5
P1
P3
P4
     0      1                             6                                       16                    18    19
Round Robin
P1
P2
P3
P4
P5
P1
P3
P5
P1
P5
P1
P5
P1
P5
P1
P1
P1
P1
P1
0     1      2     3      4      5      6      7      8      9     10   11   12     13   14     15    16   17    18      19

        b. What is the turnaround time of each process for each of the scheduling algorithms in part a?
Turnaround time = Completion time – Arrival time 
FCFS
P1, 10 – 0 = 10, P2, 11 – 0 = 11, P3, 13 – 0 = 13, P4, 14 – 0 = 14, P5, 19 – 0 = 19
SJF 
P1, 19 – 0 = 19, P2, 1 – 0 = 1, P3, 4 – 0 = 4, P4, 2 – 0 = 2, P5, 9 – 0 = 9
No preemptive Priority
P1, 16 – 0 = 16, P2, 1 – 0 = 1, P3, 18 – 0 = 18, P4, 19 – 0 = 19, P5, 6 – 0 = 6
Round Robin
P1, 19 – 0 = 19, P2, 2 – 0 = 2, P3, 7 – 0 = 7, P4, 4 – 0 = 4, P5, 14 – 0 = 14
        c. What is the waiting time of each process for each of the scheduling algorithms in part a?
Waiting time = Turnaround time – Burst time
FCFS
P1, 10 – 10 = 0, P2, 11 – 1 = 10, P3, 13 – 2 = 11, P4, 14 – 1 = 13, P5, 19 – 5 = 14
SJF 
P1, 19 – 10 = 9, P2, 1 – 1 = 0, P3, 4 – 2 = 2, P4, 2 – 1 = 1, P5, 9 – 5 = 4
No preemptive Priority
P1, 16 – 10 = 6, P2, 1 – 1 = 0, P3, 18 – 2 = 16, P4, 19 – 1 = 18, P5, 6 – 5 = 1
Round Robin
P1, 19 – 10 = 9, P2, 2 – 1 = 1, P3, 7 – 2 = 5, P4, 4 – 1 = 3, P5, 14 – 5 = 9
        d. Which of the schedules in part a result in the minimal average waiting time (over all processes)?
FCFS: 0+10+11+13+14 = 48/5 = 9.6
SJF: 9+0+2+1+4 = 16/5 = 3.2
No preemptive Priority: 6+0+16+18+1 = 41/5 = 8.2
Round Robin: 9+1+5+3+9 = 27/5 = 5.4
The shortest average waiting time is Shortest Job First (SJF) at an average of 3.2 milliseconds
    20. What advantage is there in having different time-quantum sizes on different levels of a multilevel queuing system? 

Because each queue must have more priority over lower-priority queues in a multi level queuing system, the different time-quantum sizes help in distinguishing between the queues by priority and determining the CPU time slices each queue will have. For instance

    21. Many CPU scheduling algorithms are parameterized. For example, the RR algorithm requires a parameter to indicate the time slice. Multilevel feedback queues require parameters to define the number of queues, the scheduling algorithms for each queue, the criteria used to move processes between queues, and so on. These algorithms are thus really sets of algorithms (for example, the set of RR algorithms for all time slices, and so on). One set of algorithms may include another (for example, the FCFS algorithm is the RR algorithm with an infinite time quantum). What (if any) relation holds between the following pairs of sets of algorithms? 
        a. Priority and SJF 
The SJF algorithm is a special case of the general priority-scheduling algorithm. A priority is associated with each process, and the CPU is allocated to a process with the highest priority. Equal priority processes are scheduled in FCFS order.
An SJF algorithm is simply a priority algorithm where the priority p is simply the inverse of the predicted next CPU burst time. The larger the burst time the lower the priority, and vice versa.
        b. Multilevel feedback queues and FCFS 
Processes in the lowest priority queues are scheduled in a FCFS order
        c. Priority and FCFS 
Equal priority processes are scheduled in a FCFS order.
        d. RR and SJF 
No relationship
    22. Describe the differences among short-term, medium-term, and long-term scheduling.

Short Answer
    • Short-term scheduling - also called "CPU scheduler", allocates CPU time to processes in memory 
    • Medium-term scheduling - swaps processes out of memory and reinstates them, to be run, at a later time, as in a time-sharing system 
    • Long-term scheduling - also called "job scheduler" 
A common difference, other than their defined tasks, is also how frequently each scheduler is called.
Long Answer
Short term scheduling concerns with the allocation of CPU time to processes in order to meet some pre-defined system performance objectives.
Medium term scheduling is essentially concerned with memory management, hence it's very often designed as a part of the memory management subsystem of an OS. Its efficient interaction with the short term scheduler is essential for system performances, especially in virtual memory systems. This is the reason why in paged system the pager process is usually run at a very high (dispatching) priority level. 
Long term scheduling obviously controls the degree of multiprogramming in multitasking systems, following certain policies to decide whether the system can honour a new job submission or, if more than one job is submitted, which of them should be selected. The need for some form of compromise between degree of multiprogramming and throughput seems evident, especially when one considers interactive systems.

    23. Describe with the aid of a diagram the life-cycle of a process. You should describe each of the states that it can be in, and the reasons why it moves between these states. 



As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running.
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    24. What information does the operating system keep in the process control block?

Information associated with each process.
    • Process state
    • Program counter
    • CPU registers
    • CPU scheduling information
    • Memory-management information
    • Accounting information
    • I/O status information

    25. Use an example to explain the need for process synchronization.

Cooperating processes may either share a logical address space (ie. Both code and data), or be allowed to share data through files. Process synchronization is, therefore, needed to prevent data inconsistencies where concurrent access by two processes on shared data may occur.

    26. What are critical section and race conditions?

    • Critical section: A section of code for which mutual exclusion must be implemented

    • Race condition: The situation where several processes access – and manipulate shared data concurrently. The final value of the shared data depends upon which process finishes last.

    27. Describe the purpose of semaphores and how they can be used to solve synchronization problems.

Semaphores are nonnegative integer variables that are used as a flag in and OS. The semaphore signals if and when a resource is free and can be used by a process

Semaphores are synchronization tools that can be used to solve synchronization problems by using the atomic operations: wait (p) and signal (v). For instance if two concurrent processes P1 and P2 with statements S1 and S2 respectively require that S2 be executed after S1 has completed then the scheme can be implemented by allowing P1 and P2 to share a common semaphore synch, initialized to 0, and by inserting statements in P1 to allow waiting and signaling and statements in P2 to wait, the synchronization problem can be solved.


    28. How can semaphores be used to protect critical sections? Which programming errors can occur?

A semaphore is assigned to the resource, it's shared among all processes that need to access the resource, and its counter is initialized to 1. A process then waits on the semaphore upon entering the critical section for the resource, and signals on leaving it. The first process will get access. If another process arrives while the former is still in the critical section, it'll be blocked, and so will further processes. In this situation the absolute value of the counter is equal to the number of processes waiting to enter the critical section. Every process leaving the critical section will let a waiting process use the resource by signaling the semaphore.

Programming errors
    • Starvation
    • Deadlock

    29. How can a message system be used to achieve synchronization?
Messages are yet another form of shared resource, through which cooperating processes can communicate data to each other. intuitively, we can think of implementing them by means of two primitives like 
    • send(dest, msg): to send a message to a designated process, destination of the message; 
    • receive(source, msg): to receive a message from a designated process; 
Synchronization: the primitives can cause the destination or source of the message to block until the delivery is completed (successfully or not). In this case they are called blocking, and nonblocking otherwise. The reception can also be either explicitly confirmed through the use of the analog of a return receipt or not. 
    30. Explain the term deadlock.

A set of blocked processes each holding a resource and waiting to acquire a resource held by another process in the set.

    31. What are resource allocation graphs?
A set of vertices V and a set of edges E.
    • V is partitioned into two types:
        ◦ P = {P1, P2, …,Pn}, the set consisting of all the processes in the system.
        ◦ R = {R1, R2, …,Rm}, the set consisting of all resource types in the system.
    • request edge – directed edge P1 ®Rj
    • assignment edge – directed edge Rj ®Pi

    32. Name the necessary conditions for a deadlock.

Deadlock can arise if four conditions hold simultaneously.

    • Mutual exclusion:  only one process at a time can use a resource.
    • Hold and wait:  a process holding at least one resource is waiting to acquire additional resources held by other processes.
    • No preemption:  a resource can be released only voluntarily by the process holding it, after that process has completed its task.
    • Circular wait:  there exists a set {P0, P1, …, P0} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for a resource that is held by P2, …, Pn–1 is waiting for a resource that is held by 
Pn, and P0 is waiting for a resource that is held by P0.

    33. Can deadlocks be prevented?

Yes, deadlocks can be prevented when you ensure that at least one of the four conditions cannot hold

    • Mutual Exclusion – not required for sharable resources; must hold for nonsharable resources.
    • Hold and Wait – must guarantee that whenever a process requests a resource, it does not hold any other resources.
        ◦ Require process to request and be allocated all its resources before it begins execution, or allow process to request resources only when the process has none.
        ◦ Low resource utilization; starvation possible.
    • No Preemption –
        ◦ If a process that is holding some resources requests another resource that cannot be immediately allocated to it, then all resources currently being held are released.
        ◦ Preempted resources are added to the list of resources for which the process is waiting.
        ◦ Process will be restarted only when it can regain its old resources, as well as the new ones that it is requesting.
    • Circular Wait – impose a total ordering of all resource types, and require that each process requests resources in an increasing order of enumeration.

    34. Is it possible to detect deadlocks? If yes, how can detected deadlocks be resolved?

Yes, deadlocks can be detected using a detection algorithm (Periodically invoke an algorithm that searches for the deadlock), if the resource allocation graphs contains no cycles, then there are no deadlocks, if a graph contains a cycle: i.e. if only one instance per resource type, then there is a deadlock, if several instances per resource type there is a possibility of a deadlock
	
Deadlocks can be resolved in the following ways
    • Abort all deadlocked processes
    • Abort one process at a time until the deadlock cycle is eliminated.

    3. Memory

    35. Describe the following memory allocation algorithms: 
        a. First fit: Allocate the first hole that is big enough. 
        b. Best fit: Allocate the smallest hole that is big enough; must search entire list, unless ordered by size. Produces the smallest leftover hole.
        c. Worst fit: Allocate the largest hole; must also search entire list. Produces the largest leftover hole.
    36. Explain the term “swapping”.

Swapping is a mechanism of removing a process temporarily out of memory to a backing store and then, and then brought back into memory for continued execution.

Example: Assume a multiprogramming environment with a round robin CPU scheduling algorithm. When a quantum expires, the memory manager will start to swap out the process that just finished, and swap in another process to the memory space that has just been freed.


    37. What is the difference between internal and external fragmentation?

    • Internal Fragmentation – allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used
    • External Fragmentation – total memory space exists to satisfy a request, but it is not contiguous; storage is fragmented into a large number of small holes

    38. Explain the term “segmentation”.

    • Memory-management scheme that supports user view of memory. 

    39. What is compaction and in which context is it used?

Compaction is referred to as garbage collection, performed by the OS to reclaim fragmented sections of the memory space. 



Its should be used when;
    • When a certain percentage of memory becomes busy, say 80%.  
    • When there are processes waiting to get in. 
    • After a prescribed amount of time has elapsed

    40. Explain the term “paging”.

Paging is a memory management scheme that permits the physical address space of a process to be noncontiguous.???

    41. What are page faults? How are they handled by an operating system?

Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 



    42. Explain the term “virtual memory”.

A technique that allows programs to be executed even though they are not stored entirely in memory. It gives a user the illusion that a large amount of main memory is available when, in fact, it is not. 

    43. Name three loading and three replacement strategies and discuss their advantages and disadvantages.













    44. Why are segmentation and paging sometimes combined into one scheme? 

To remove the problems of external fragmentation and compaction, because the pages are of fixed length

    45. In the context of memory management, under which circumstances do external and internal fragmentations occur? How can each be handled? 

Internal Fragmentation occurs when fixed partitions suffer from inefficient memory use - any process, no matter how small, occupies an entire partition.

Internal fragmentation can be solved by combining free areas of memory whenever possible and combining it to form a block that will be deallocate.

External Fragmentation occurs when process are loaded and removed from memory, the free memory space is broken down into little pieces. 

External fragmentation can be handle by compaction; the OS reclaims fragmented sections of the memory space and compacts them to make one large block of memory that is large enough to accommodate some or all of the jobs waiting to get in. 


    46. Under what circumstances do page faults occur? Describe the actions taken by the operating system when a page fault occurs. 
Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 

    47. When virtual memory is implemented in a computing system, it carries certain costs and certain benefits. List those costs and the benefits. It is possible for the costs to exceed the benefits. Explain what measures you can take to ensure that this imbalance does not occur.

Benefits
    • Users can write extremely very large programs without being concerned with the limitations of main memory. 
    • Increases multiprogramming with a corresponding increase in CPU utilization and throughput, but with no increase in response time or turnaround time. 
    • Less I/O would be needed to load or swap each user program into memory, so each user program would run faster. 

    48. What is the cause of thrashing? How does the system detect thrashing? Once it detects thrashing, what can the system do to eliminate this problem? 

Thrashing is caused when a process is busy swapping pages in and out. The process spend more time paging than executing

Thrashing can be detected by;
    • Low CPU utilization

Thrashing can be limited by using a local replacement algorithm; if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash also.

    49. What is meant by the term demand paging in a virtual memory management system, and how is it implemented?

Demand paging is swapping a process into memory when it is needed

Implementation
    • Page is needed Þ reference to it
    • invalid reference Þ abort
    • not-in-memory Þ bring to memory
    1. Introduction 
    1. Describe the main purpose of an operating system.

The main purpose of an OS is to act as an intermediary between the user of a computer and the computer hardware by providing an environment in which a user can execute programs in a convenient and efficient manner.

    2. Identify 3 activities of an operating system in respect to process management, memory management, I/O management and file management. 
	Process Management:
        ◦ Process creation and deletion.
        ◦ Process suspension and resumption.
        ◦ Provision of mechanisms for:
            i. process synchronization
            ii. process communication
	Memory Management:
                • Keep track of which parts of memory are currently being used and by whom.
                • Decide which processes to load when memory space becomes available.
                • Allocate and deallocate memory space as needed.
	File Management:
    • File creation and deletion.
    • Directory creation and deletion.
    • Support of primitives for manipulating files and directories.
    • Mapping files onto secondary storage.
    • File backup on stable (nonvolatile) storage media.
	I/O Management:
        ◦ A buffer-caching system 
        ◦ A general device-driver interface
        ◦ Drivers for specific hardware devices
	Secondary Storage Management:
    • Free space management
    • Storage allocation
    • Disk scheduling
	
    3. Name two storage media with random access and two media with sequential access.
	Random Access:
    • RAM
    • Registers
    • Cache
	Sequential Access:
    • Magnetic Disks???
    • Optical Disks
    • Magnetic Tapes

    4. What are the differences between a trap and an interrupt? What is the use of each function? 

	A trap is a software-generated interrupt caused either by an error or a specific 	user program request.
An Interrupt is an asynchronous break in program flow that occurs as a result of events outside the running program. It’s usually hardware related, stemming from events such as a button press, timer expiration, or completion of a data transfer. An Interrupt triggers execution of instructions that perform work on behalf of the system, but not necessarily the current program. They were developed for exception handling and were later applied to I/O events.
    5. What are the purposes of system calls and system programs?

System calls provide an interface between the process and the operating system. System calls allow user-level processes to request some services from the operating system which process itself is not allowed to do. For example, for I/O a process involves a system call telling the operating system to read or write particular area and this request is satisfied by the operating system.
	
The purpose of a system program is to provide a convenient environment for   program development and execution.  


    6. What are the main advantages of multiprogramming?

    • Increases CPU utilization by organizing  jobs so that the CPU always has one to execute
    • Allows time sharing: many users can use a computer system interactively at the same time

	
    7. What is multi tasking, multi programming, multi threading? 

Multi tasking: The ability to execute more than one task at the same time

Multi programming: The ability to run several programs on a uniprocessor.

Multi threading:The ability of an operating system to execute different parts of a program, called threads, simultaneously. ***??

    8. What is a Real-Time System? 

A real-time system is one with well defined, fixed time constraints whereby processing must be done within the defined constraints or the system will fail


    9. What is the difference between Hard and Soft real-time systems? What is a mission critical system?
    • Hard real-time
        ◦ Secondary storage limited or absent, data stored in short term memory, or read-only memory (ROM)
        ◦ Conflicts with time-sharing systems, not supported by general-purpose operating systems.
        ◦ Guarantees that all critical tasks be completed on time
    • Soft real-time
        ◦ Limited utility in industrial control of robotics
        ◦ Useful in applications (multimedia, virtual reality) requiring advanced operating-system features.
        ◦ A critical real-time task gets priority over other tasks and retains that priority until it completes

    • Mission Critical System: 
	


    2. Processes

    10. Explain the difference between a program and a process.

A process is a program loaded into memory and executing. It is a unit of work in a system. 

A program is lines of code or algorithm expressed in some suitable notation.

    11. What are typical process states and typical transitions between these states.

	As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • Waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.



 


Transitions between the states

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → Waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out  (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running. 
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    12. Why are program counter and processor registers part of the process control block?
The process control block represents each process in the O/S. 

    • The program counter contains the address of the next instruction for execution for 	this process
    • The processor registers are a volatile storage medium that are used to store the current state of a process when an interrupt occurs. They allow the process to be 	continued correctly afterwards 

    13. Explain the main differences between processes and threads.
Differences
    • Unlike processes, threads are not independent of one another. 
    • Unlike processes, all threads can access every address in the task. 
    • Unlike processes, threads are designed to assist one other. Note that processes might or might not assist one another because processes may originate from different users. 
	Similarities
    • Like processes threads share CPU and only one thread active (running) at a time. 
    • Like processes, threads within a process, execute sequentially. 
    • Like processes, threads can create children. 
    • And like process, if one thread is blocked, another thread can run. 

    14. What are the requirements for a good scheduler?

    • Max CPU utilization – keep the CPU as busy as possible
    • Max Throughput – Number of processes that complete their execution per time unit
    • Min Turnaround time – amount of time to execute a particular process
    • Min Waiting time – amount of time a process has been waiting in the ready queue
    • Min Response time – amount of time it takes from when a request was submitted until the first response is produced, not output  (for time-sharing environment)

    15. Explain the difference between preemptive and non-preemptive schedulers.

    • Nonpreemptive – once CPU given to the process it cannot be interrupted until it completes its CPU burst or switches to a waiting state.
    • Preemptive – if a new process arrives with CPU burst length less than remaining time of current executing process, preempt.  This scheme is known as the Shortest-Remaining-Time-First (SRTF).

Explain why strict nonpreemptive scheduling is unlikely to be used in a computer center.
	Convoy effect, short jobs are made to wait by longer jobs

    16. Explain the scheduling strategies FCFS, LPTF, SJF, SRTF, and RR.

    • FCFS: (First Come First Served) process that requests the CPU first, gets the CPU first (FIFO)

    • SJF: (Shortest Job First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the smallest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    • SRTF: (Shortest Remaining Time First) pre-emptive SJF scheduling: When a process with a shorter next CPU burst than what is left of the currently executing process arrives in the ready queue, it will preempt the currently executing process

    • RR: (Round Robin scheduling) designed especially for timesharing systems. Similar to FCFS but preemption is added to switch between processes. A small unit called a time quantum or time slice is defined. The ready queue is treated as a circular queue. The CPU scheduler goes around the ready queue allocating the CPU for each process for a time interval of up to one time quantum


    • LPTF: (Long Processing Time First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the longest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    17. What are two differences between user-level threads and kernel-level threads? Under what circumstances is one type better than the another?

    • User-level threads are faster to create and require less overhead than kernel-level threads.
    • The kernel knows about kernel-level threads, but not user-level threads, so K/L threads can be scheduled by the kernel’s scheduler.

K/L threads are better than U/L threads when you will have threads that block, for example, on I/O. In these cases, the scheduler can choose another K/L to run so that the whole process doesn’t block.

U/L threads are better when there will be a lot of them created, or when they will be created and destroyed relatively quickly because there is little creation/destruction overhead. They are also better when the threads need to communicate because thread communication is simply a procedure call in U/L threads, which is faster than going through the kernel.

Short Answer
    • User threads are built using a library and are unknown or undifferentiated by the kernel. 
    • Kernel threads require more operating system resources because they are managed by the kernel. 
A user thread is more appropriate for low-level tasks, whereas a kernel thread is better for high-priority tasks that should get high priority to system resources

    18. What resources are used when a thread is created? How do they differ from those used when a process is created?

When a thread is created, it needs its own execution state. This basically involves allocating it memory for its stack, and giving it a PC, SP, and storage for its other registers, all within the processes address space.

When a process is created, it receives an entirely new address space, which means that the system has to create a new mapping from addresses to physical memory, and it has to allocate the process a new block of physical memory. Within that address space, the system must keep track of the open files, the process’s permissions, etc. It must also allocate things that are required for each thread within that process to have its own execution state, including a place in memory to store the stack and the registers for each thread in the process, as listed above.

So the resources used in creating a thread are a strict (and much smaller) subset of those used when creating a process.

Short Answer
When created, a thread requires an ID, a register set, a stack and program counter. 
Threads are smaller than processes. In addition to the thread information, each process creates its own Process Control Block (PCB), containing a code section, data section, and O/S resources. Threads reference these items from their creating or parent process.

    19. Consider the following set of processes, with the length of the CPU-burst time given in milliseconds: 
Process
Burst Time 
Priority
P1
10
3
P2
1
1
P3
2
3
P4
1
4
P5
5
2
The processes are assumed to have arrived in the order PI, P2, P3, P4, P5, all at time 0 
        a. Draw four Gantt charts that illustrate the execution of these processes using FCFS, SJF, a nonpreemptive priority (a smaller priority number implies a higher priority), and RR (quantum = 1) scheduling.
FCFS
P1
P2
P3
P4
P5
	          0			           10     11              13     14                             19
		      SJF	
P2
P4
P3
P5
P1
		           0      1	     2                4                               9                                              19


No preemptive Priority
P2
P5
P1
P3
P4
     0      1                             6                                       16                    18    19
Round Robin
P1
P2
P3
P4
P5
P1
P3
P5
P1
P5
P1
P5
P1
P5
P1
P1
P1
P1
P1
0     1      2     3      4      5      6      7      8      9     10   11   12     13   14     15    16   17    18      19

        b. What is the turnaround time of each process for each of the scheduling algorithms in part a?
Turnaround time = Completion time – Arrival time 
FCFS
P1, 10 – 0 = 10, P2, 11 – 0 = 11, P3, 13 – 0 = 13, P4, 14 – 0 = 14, P5, 19 – 0 = 19
SJF 
P1, 19 – 0 = 19, P2, 1 – 0 = 1, P3, 4 – 0 = 4, P4, 2 – 0 = 2, P5, 9 – 0 = 9
No preemptive Priority
P1, 16 – 0 = 16, P2, 1 – 0 = 1, P3, 18 – 0 = 18, P4, 19 – 0 = 19, P5, 6 – 0 = 6
Round Robin
P1, 19 – 0 = 19, P2, 2 – 0 = 2, P3, 7 – 0 = 7, P4, 4 – 0 = 4, P5, 14 – 0 = 14
        c. What is the waiting time of each process for each of the scheduling algorithms in part a?
Waiting time = Turnaround time – Burst time
FCFS
P1, 10 – 10 = 0, P2, 11 – 1 = 10, P3, 13 – 2 = 11, P4, 14 – 1 = 13, P5, 19 – 5 = 14
SJF 
P1, 19 – 10 = 9, P2, 1 – 1 = 0, P3, 4 – 2 = 2, P4, 2 – 1 = 1, P5, 9 – 5 = 4
No preemptive Priority
P1, 16 – 10 = 6, P2, 1 – 1 = 0, P3, 18 – 2 = 16, P4, 19 – 1 = 18, P5, 6 – 5 = 1
Round Robin
P1, 19 – 10 = 9, P2, 2 – 1 = 1, P3, 7 – 2 = 5, P4, 4 – 1 = 3, P5, 14 – 5 = 9
        d. Which of the schedules in part a result in the minimal average waiting time (over all processes)?
FCFS: 0+10+11+13+14 = 48/5 = 9.6
SJF: 9+0+2+1+4 = 16/5 = 3.2
No preemptive Priority: 6+0+16+18+1 = 41/5 = 8.2
Round Robin: 9+1+5+3+9 = 27/5 = 5.4
The shortest average waiting time is Shortest Job First (SJF) at an average of 3.2 milliseconds
    20. What advantage is there in having different time-quantum sizes on different levels of a multilevel queuing system? 

Because each queue must have more priority over lower-priority queues in a multi level queuing system, the different time-quantum sizes help in distinguishing between the queues by priority and determining the CPU time slices each queue will have. For instance

    21. Many CPU scheduling algorithms are parameterized. For example, the RR algorithm requires a parameter to indicate the time slice. Multilevel feedback queues require parameters to define the number of queues, the scheduling algorithms for each queue, the criteria used to move processes between queues, and so on. These algorithms are thus really sets of algorithms (for example, the set of RR algorithms for all time slices, and so on). One set of algorithms may include another (for example, the FCFS algorithm is the RR algorithm with an infinite time quantum). What (if any) relation holds between the following pairs of sets of algorithms? 
        a. Priority and SJF 
The SJF algorithm is a special case of the general priority-scheduling algorithm. A priority is associated with each process, and the CPU is allocated to a process with the highest priority. Equal priority processes are scheduled in FCFS order.
An SJF algorithm is simply a priority algorithm where the priority p is simply the inverse of the predicted next CPU burst time. The larger the burst time the lower the priority, and vice versa.
        b. Multilevel feedback queues and FCFS 
Processes in the lowest priority queues are scheduled in a FCFS order
        c. Priority and FCFS 
Equal priority processes are scheduled in a FCFS order.
        d. RR and SJF 
No relationship
    22. Describe the differences among short-term, medium-term, and long-term scheduling.

Short Answer
    • Short-term scheduling - also called "CPU scheduler", allocates CPU time to processes in memory 
    • Medium-term scheduling - swaps processes out of memory and reinstates them, to be run, at a later time, as in a time-sharing system 
    • Long-term scheduling - also called "job scheduler" 
A common difference, other than their defined tasks, is also how frequently each scheduler is called.
Long Answer
Short term scheduling concerns with the allocation of CPU time to processes in order to meet some pre-defined system performance objectives.
Medium term scheduling is essentially concerned with memory management, hence it's very often designed as a part of the memory management subsystem of an OS. Its efficient interaction with the short term scheduler is essential for system performances, especially in virtual memory systems. This is the reason why in paged system the pager process is usually run at a very high (dispatching) priority level. 
Long term scheduling obviously controls the degree of multiprogramming in multitasking systems, following certain policies to decide whether the system can honour a new job submission or, if more than one job is submitted, which of them should be selected. The need for some form of compromise between degree of multiprogramming and throughput seems evident, especially when one considers interactive systems.

    23. Describe with the aid of a diagram the life-cycle of a process. You should describe each of the states that it can be in, and the reasons why it moves between these states. 



As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running.
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    24. What information does the operating system keep in the process control block?

Information associated with each process.
    • Process state
    • Program counter
    • CPU registers
    • CPU scheduling information
    • Memory-management information
    • Accounting information
    • I/O status information

    25. Use an example to explain the need for process synchronization.

Cooperating processes may either share a logical address space (ie. Both code and data), or be allowed to share data through files. Process synchronization is, therefore, needed to prevent data inconsistencies where concurrent access by two processes on shared data may occur.

    26. What are critical section and race conditions?

    • Critical section: A section of code for which mutual exclusion must be implemented

    • Race condition: The situation where several processes access – and manipulate shared data concurrently. The final value of the shared data depends upon which process finishes last.

    27. Describe the purpose of semaphores and how they can be used to solve synchronization problems.

Semaphores are nonnegative integer variables that are used as a flag in and OS. The semaphore signals if and when a resource is free and can be used by a process

Semaphores are synchronization tools that can be used to solve synchronization problems by using the atomic operations: wait (p) and signal (v). For instance if two concurrent processes P1 and P2 with statements S1 and S2 respectively require that S2 be executed after S1 has completed then the scheme can be implemented by allowing P1 and P2 to share a common semaphore synch, initialized to 0, and by inserting statements in P1 to allow waiting and signaling and statements in P2 to wait, the synchronization problem can be solved.


    28. How can semaphores be used to protect critical sections? Which programming errors can occur?

A semaphore is assigned to the resource, it's shared among all processes that need to access the resource, and its counter is initialized to 1. A process then waits on the semaphore upon entering the critical section for the resource, and signals on leaving it. The first process will get access. If another process arrives while the former is still in the critical section, it'll be blocked, and so will further processes. In this situation the absolute value of the counter is equal to the number of processes waiting to enter the critical section. Every process leaving the critical section will let a waiting process use the resource by signaling the semaphore.

Programming errors
    • Starvation
    • Deadlock

    29. How can a message system be used to achieve synchronization?
Messages are yet another form of shared resource, through which cooperating processes can communicate data to each other. intuitively, we can think of implementing them by means of two primitives like 
    • send(dest, msg): to send a message to a designated process, destination of the message; 
    • receive(source, msg): to receive a message from a designated process; 
Synchronization: the primitives can cause the destination or source of the message to block until the delivery is completed (successfully or not). In this case they are called blocking, and nonblocking otherwise. The reception can also be either explicitly confirmed through the use of the analog of a return receipt or not. 
    30. Explain the term deadlock.

A set of blocked processes each holding a resource and waiting to acquire a resource held by another process in the set.

    31. What are resource allocation graphs?
A set of vertices V and a set of edges E.
    • V is partitioned into two types:
        ◦ P = {P1, P2, …,Pn}, the set consisting of all the processes in the system.
        ◦ R = {R1, R2, …,Rm}, the set consisting of all resource types in the system.
    • request edge – directed edge P1 ®Rj
    • assignment edge – directed edge Rj ®Pi

    32. Name the necessary conditions for a deadlock.

Deadlock can arise if four conditions hold simultaneously.

    • Mutual exclusion:  only one process at a time can use a resource.
    • Hold and wait:  a process holding at least one resource is waiting to acquire additional resources held by other processes.
    • No preemption:  a resource can be released only voluntarily by the process holding it, after that process has completed its task.
    • Circular wait:  there exists a set {P0, P1, …, P0} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for a resource that is held by P2, …, Pn–1 is waiting for a resource that is held by 
Pn, and P0 is waiting for a resource that is held by P0.

    33. Can deadlocks be prevented?

Yes, deadlocks can be prevented when you ensure that at least one of the four conditions cannot hold

    • Mutual Exclusion – not required for sharable resources; must hold for nonsharable resources.
    • Hold and Wait – must guarantee that whenever a process requests a resource, it does not hold any other resources.
        ◦ Require process to request and be allocated all its resources before it begins execution, or allow process to request resources only when the process has none.
        ◦ Low resource utilization; starvation possible.
    • No Preemption –
        ◦ If a process that is holding some resources requests another resource that cannot be immediately allocated to it, then all resources currently being held are released.
        ◦ Preempted resources are added to the list of resources for which the process is waiting.
        ◦ Process will be restarted only when it can regain its old resources, as well as the new ones that it is requesting.
    • Circular Wait – impose a total ordering of all resource types, and require that each process requests resources in an increasing order of enumeration.

    34. Is it possible to detect deadlocks? If yes, how can detected deadlocks be resolved?

Yes, deadlocks can be detected using a detection algorithm (Periodically invoke an algorithm that searches for the deadlock), if the resource allocation graphs contains no cycles, then there are no deadlocks, if a graph contains a cycle: i.e. if only one instance per resource type, then there is a deadlock, if several instances per resource type there is a possibility of a deadlock
	
Deadlocks can be resolved in the following ways
    • Abort all deadlocked processes
    • Abort one process at a time until the deadlock cycle is eliminated.

    3. Memory

    35. Describe the following memory allocation algorithms: 
        a. First fit: Allocate the first hole that is big enough. 
        b. Best fit: Allocate the smallest hole that is big enough; must search entire list, unless ordered by size. Produces the smallest leftover hole.
        c. Worst fit: Allocate the largest hole; must also search entire list. Produces the largest leftover hole.
    36. Explain the term “swapping”.

Swapping is a mechanism of removing a process temporarily out of memory to a backing store and then, and then brought back into memory for continued execution.

Example: Assume a multiprogramming environment with a round robin CPU scheduling algorithm. When a quantum expires, the memory manager will start to swap out the process that just finished, and swap in another process to the memory space that has just been freed.


    37. What is the difference between internal and external fragmentation?

    • Internal Fragmentation – allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used
    • External Fragmentation – total memory space exists to satisfy a request, but it is not contiguous; storage is fragmented into a large number of small holes

    38. Explain the term “segmentation”.

    • Memory-management scheme that supports user view of memory. 

    39. What is compaction and in which context is it used?

Compaction is referred to as garbage collection, performed by the OS to reclaim fragmented sections of the memory space. 



Its should be used when;
    • When a certain percentage of memory becomes busy, say 80%.  
    • When there are processes waiting to get in. 
    • After a prescribed amount of time has elapsed

    40. Explain the term “paging”.

Paging is a memory management scheme that permits the physical address space of a process to be noncontiguous.???

    41. What are page faults? How are they handled by an operating system?

Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 



    42. Explain the term “virtual memory”.

A technique that allows programs to be executed even though they are not stored entirely in memory. It gives a user the illusion that a large amount of main memory is available when, in fact, it is not. 

    43. Name three loading and three replacement strategies and discuss their advantages and disadvantages.













    44. Why are segmentation and paging sometimes combined into one scheme? 

To remove the problems of external fragmentation and compaction, because the pages are of fixed length

    45. In the context of memory management, under which circumstances do external and internal fragmentations occur? How can each be handled? 

Internal Fragmentation occurs when fixed partitions suffer from inefficient memory use - any process, no matter how small, occupies an entire partition.

Internal fragmentation can be solved by combining free areas of memory whenever possible and combining it to form a block that will be deallocate.

External Fragmentation occurs when process are loaded and removed from memory, the free memory space is broken down into little pieces. 

External fragmentation can be handle by compaction; the OS reclaims fragmented sections of the memory space and compacts them to make one large block of memory that is large enough to accommodate some or all of the jobs waiting to get in. 


    46. Under what circumstances do page faults occur? Describe the actions taken by the operating system when a page fault occurs. 
Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 

    47. When virtual memory is implemented in a computing system, it carries certain costs and certain benefits. List those costs and the benefits. It is possible for the costs to exceed the benefits. Explain what measures you can take to ensure that this imbalance does not occur.

Benefits
    • Users can write extremely very large programs without being concerned with the limitations of main memory. 
    • Increases multiprogramming with a corresponding increase in CPU utilization and throughput, but with no increase in response time or turnaround time. 
    • Less I/O would be needed to load or swap each user program into memory, so each user program would run faster. 

    48. What is the cause of thrashing? How does the system detect thrashing? Once it detects thrashing, what can the system do to eliminate this problem? 

Thrashing is caused when a process is busy swapping pages in and out. The process spend more time paging than executing

Thrashing can be detected by;
    • Low CPU utilization

Thrashing can be limited by using a local replacement algorithm; if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash also.

    49. What is meant by the term demand paging in a virtual memory management system, and how is it implemented?

Demand paging is swapping a process into memory when it is needed

Implementation
    • Page is needed Þ reference to it
    • invalid reference Þ abort
    • not-in-memory Þ bring to memory
    1. Introduction 
    1. Describe the main purpose of an operating system.

The main purpose of an OS is to act as an intermediary between the user of a computer and the computer hardware by providing an environment in which a user can execute programs in a convenient and efficient manner.

    2. Identify 3 activities of an operating system in respect to process management, memory management, I/O management and file management. 
	Process Management:
        ◦ Process creation and deletion.
        ◦ Process suspension and resumption.
        ◦ Provision of mechanisms for:
            i. process synchronization
            ii. process communication
	Memory Management:
                • Keep track of which parts of memory are currently being used and by whom.
                • Decide which processes to load when memory space becomes available.
                • Allocate and deallocate memory space as needed.
	File Management:
    • File creation and deletion.
    • Directory creation and deletion.
    • Support of primitives for manipulating files and directories.
    • Mapping files onto secondary storage.
    • File backup on stable (nonvolatile) storage media.
	I/O Management:
        ◦ A buffer-caching system 
        ◦ A general device-driver interface
        ◦ Drivers for specific hardware devices
	Secondary Storage Management:
    • Free space management
    • Storage allocation
    • Disk scheduling
	
    3. Name two storage media with random access and two media with sequential access.
	Random Access:
    • RAM
    • Registers
    • Cache
	Sequential Access:
    • Magnetic Disks???
    • Optical Disks
    • Magnetic Tapes

    4. What are the differences between a trap and an interrupt? What is the use of each function? 

	A trap is a software-generated interrupt caused either by an error or a specific 	user program request.
An Interrupt is an asynchronous break in program flow that occurs as a result of events outside the running program. It’s usually hardware related, stemming from events such as a button press, timer expiration, or completion of a data transfer. An Interrupt triggers execution of instructions that perform work on behalf of the system, but not necessarily the current program. They were developed for exception handling and were later applied to I/O events.
    5. What are the purposes of system calls and system programs?

System calls provide an interface between the process and the operating system. System calls allow user-level processes to request some services from the operating system which process itself is not allowed to do. For example, for I/O a process involves a system call telling the operating system to read or write particular area and this request is satisfied by the operating system.
	
The purpose of a system program is to provide a convenient environment for   program development and execution.  


    6. What are the main advantages of multiprogramming?

    • Increases CPU utilization by organizing  jobs so that the CPU always has one to execute
    • Allows time sharing: many users can use a computer system interactively at the same time

	
    7. What is multi tasking, multi programming, multi threading? 

Multi tasking: The ability to execute more than one task at the same time

Multi programming: The ability to run several programs on a uniprocessor.

Multi threading:The ability of an operating system to execute different parts of a program, called threads, simultaneously. ***??

    8. What is a Real-Time System? 

A real-time system is one with well defined, fixed time constraints whereby processing must be done within the defined constraints or the system will fail


    9. What is the difference between Hard and Soft real-time systems? What is a mission critical system?
    • Hard real-time
        ◦ Secondary storage limited or absent, data stored in short term memory, or read-only memory (ROM)
        ◦ Conflicts with time-sharing systems, not supported by general-purpose operating systems.
        ◦ Guarantees that all critical tasks be completed on time
    • Soft real-time
        ◦ Limited utility in industrial control of robotics
        ◦ Useful in applications (multimedia, virtual reality) requiring advanced operating-system features.
        ◦ A critical real-time task gets priority over other tasks and retains that priority until it completes

    • Mission Critical System: 
	


    2. Processes

    10. Explain the difference between a program and a process.

A process is a program loaded into memory and executing. It is a unit of work in a system. 

A program is lines of code or algorithm expressed in some suitable notation.

    11. What are typical process states and typical transitions between these states.

	As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • Waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.



 


Transitions between the states

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → Waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out  (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running. 
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    12. Why are program counter and processor registers part of the process control block?
The process control block represents each process in the O/S. 

    • The program counter contains the address of the next instruction for execution for 	this process
    • The processor registers are a volatile storage medium that are used to store the current state of a process when an interrupt occurs. They allow the process to be 	continued correctly afterwards 

    13. Explain the main differences between processes and threads.
Differences
    • Unlike processes, threads are not independent of one another. 
    • Unlike processes, all threads can access every address in the task. 
    • Unlike processes, threads are designed to assist one other. Note that processes might or might not assist one another because processes may originate from different users. 
	Similarities
    • Like processes threads share CPU and only one thread active (running) at a time. 
    • Like processes, threads within a process, execute sequentially. 
    • Like processes, threads can create children. 
    • And like process, if one thread is blocked, another thread can run. 

    14. What are the requirements for a good scheduler?

    • Max CPU utilization – keep the CPU as busy as possible
    • Max Throughput – Number of processes that complete their execution per time unit
    • Min Turnaround time – amount of time to execute a particular process
    • Min Waiting time – amount of time a process has been waiting in the ready queue
    • Min Response time – amount of time it takes from when a request was submitted until the first response is produced, not output  (for time-sharing environment)

    15. Explain the difference between preemptive and non-preemptive schedulers.

    • Nonpreemptive – once CPU given to the process it cannot be interrupted until it completes its CPU burst or switches to a waiting state.
    • Preemptive – if a new process arrives with CPU burst length less than remaining time of current executing process, preempt.  This scheme is known as the Shortest-Remaining-Time-First (SRTF).

Explain why strict nonpreemptive scheduling is unlikely to be used in a computer center.
	Convoy effect, short jobs are made to wait by longer jobs

    16. Explain the scheduling strategies FCFS, LPTF, SJF, SRTF, and RR.

    • FCFS: (First Come First Served) process that requests the CPU first, gets the CPU first (FIFO)

    • SJF: (Shortest Job First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the smallest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    • SRTF: (Shortest Remaining Time First) pre-emptive SJF scheduling: When a process with a shorter next CPU burst than what is left of the currently executing process arrives in the ready queue, it will preempt the currently executing process

    • RR: (Round Robin scheduling) designed especially for timesharing systems. Similar to FCFS but preemption is added to switch between processes. A small unit called a time quantum or time slice is defined. The ready queue is treated as a circular queue. The CPU scheduler goes around the ready queue allocating the CPU for each process for a time interval of up to one time quantum


    • LPTF: (Long Processing Time First) Algorithm associates with each process, the length of the process’ next CPU burst. When the CPU is available, it is assigned to the process that has the longest next CPU burst. If 2 processes have the same next CPU burst, then FCFS scheduling will be used to break the tie.

    17. What are two differences between user-level threads and kernel-level threads? Under what circumstances is one type better than the another?

    • User-level threads are faster to create and require less overhead than kernel-level threads.
    • The kernel knows about kernel-level threads, but not user-level threads, so K/L threads can be scheduled by the kernel’s scheduler.

K/L threads are better than U/L threads when you will have threads that block, for example, on I/O. In these cases, the scheduler can choose another K/L to run so that the whole process doesn’t block.

U/L threads are better when there will be a lot of them created, or when they will be created and destroyed relatively quickly because there is little creation/destruction overhead. They are also better when the threads need to communicate because thread communication is simply a procedure call in U/L threads, which is faster than going through the kernel.

Short Answer
    • User threads are built using a library and are unknown or undifferentiated by the kernel. 
    • Kernel threads require more operating system resources because they are managed by the kernel. 
A user thread is more appropriate for low-level tasks, whereas a kernel thread is better for high-priority tasks that should get high priority to system resources

    18. What resources are used when a thread is created? How do they differ from those used when a process is created?

When a thread is created, it needs its own execution state. This basically involves allocating it memory for its stack, and giving it a PC, SP, and storage for its other registers, all within the processes address space.

When a process is created, it receives an entirely new address space, which means that the system has to create a new mapping from addresses to physical memory, and it has to allocate the process a new block of physical memory. Within that address space, the system must keep track of the open files, the process’s permissions, etc. It must also allocate things that are required for each thread within that process to have its own execution state, including a place in memory to store the stack and the registers for each thread in the process, as listed above.

So the resources used in creating a thread are a strict (and much smaller) subset of those used when creating a process.

Short Answer
When created, a thread requires an ID, a register set, a stack and program counter. 
Threads are smaller than processes. In addition to the thread information, each process creates its own Process Control Block (PCB), containing a code section, data section, and O/S resources. Threads reference these items from their creating or parent process.

    19. Consider the following set of processes, with the length of the CPU-burst time given in milliseconds: 
Process
Burst Time 
Priority
P1
10
3
P2
1
1
P3
2
3
P4
1
4
P5
5
2
The processes are assumed to have arrived in the order PI, P2, P3, P4, P5, all at time 0 
        a. Draw four Gantt charts that illustrate the execution of these processes using FCFS, SJF, a nonpreemptive priority (a smaller priority number implies a higher priority), and RR (quantum = 1) scheduling.
FCFS
P1
P2
P3
P4
P5
	          0			           10     11              13     14                             19
		      SJF	
P2
P4
P3
P5
P1
		           0      1	     2                4                               9                                              19


No preemptive Priority
P2
P5
P1
P3
P4
     0      1                             6                                       16                    18    19
Round Robin
P1
P2
P3
P4
P5
P1
P3
P5
P1
P5
P1
P5
P1
P5
P1
P1
P1
P1
P1
0     1      2     3      4      5      6      7      8      9     10   11   12     13   14     15    16   17    18      19

        b. What is the turnaround time of each process for each of the scheduling algorithms in part a?
Turnaround time = Completion time – Arrival time 
FCFS
P1, 10 – 0 = 10, P2, 11 – 0 = 11, P3, 13 – 0 = 13, P4, 14 – 0 = 14, P5, 19 – 0 = 19
SJF 
P1, 19 – 0 = 19, P2, 1 – 0 = 1, P3, 4 – 0 = 4, P4, 2 – 0 = 2, P5, 9 – 0 = 9
No preemptive Priority
P1, 16 – 0 = 16, P2, 1 – 0 = 1, P3, 18 – 0 = 18, P4, 19 – 0 = 19, P5, 6 – 0 = 6
Round Robin
P1, 19 – 0 = 19, P2, 2 – 0 = 2, P3, 7 – 0 = 7, P4, 4 – 0 = 4, P5, 14 – 0 = 14
        c. What is the waiting time of each process for each of the scheduling algorithms in part a?
Waiting time = Turnaround time – Burst time
FCFS
P1, 10 – 10 = 0, P2, 11 – 1 = 10, P3, 13 – 2 = 11, P4, 14 – 1 = 13, P5, 19 – 5 = 14
SJF 
P1, 19 – 10 = 9, P2, 1 – 1 = 0, P3, 4 – 2 = 2, P4, 2 – 1 = 1, P5, 9 – 5 = 4
No preemptive Priority
P1, 16 – 10 = 6, P2, 1 – 1 = 0, P3, 18 – 2 = 16, P4, 19 – 1 = 18, P5, 6 – 5 = 1
Round Robin
P1, 19 – 10 = 9, P2, 2 – 1 = 1, P3, 7 – 2 = 5, P4, 4 – 1 = 3, P5, 14 – 5 = 9
        d. Which of the schedules in part a result in the minimal average waiting time (over all processes)?
FCFS: 0+10+11+13+14 = 48/5 = 9.6
SJF: 9+0+2+1+4 = 16/5 = 3.2
No preemptive Priority: 6+0+16+18+1 = 41/5 = 8.2
Round Robin: 9+1+5+3+9 = 27/5 = 5.4
The shortest average waiting time is Shortest Job First (SJF) at an average of 3.2 milliseconds
    20. What advantage is there in having different time-quantum sizes on different levels of a multilevel queuing system? 

Because each queue must have more priority over lower-priority queues in a multi level queuing system, the different time-quantum sizes help in distinguishing between the queues by priority and determining the CPU time slices each queue will have. For instance

    21. Many CPU scheduling algorithms are parameterized. For example, the RR algorithm requires a parameter to indicate the time slice. Multilevel feedback queues require parameters to define the number of queues, the scheduling algorithms for each queue, the criteria used to move processes between queues, and so on. These algorithms are thus really sets of algorithms (for example, the set of RR algorithms for all time slices, and so on). One set of algorithms may include another (for example, the FCFS algorithm is the RR algorithm with an infinite time quantum). What (if any) relation holds between the following pairs of sets of algorithms? 
        a. Priority and SJF 
The SJF algorithm is a special case of the general priority-scheduling algorithm. A priority is associated with each process, and the CPU is allocated to a process with the highest priority. Equal priority processes are scheduled in FCFS order.
An SJF algorithm is simply a priority algorithm where the priority p is simply the inverse of the predicted next CPU burst time. The larger the burst time the lower the priority, and vice versa.
        b. Multilevel feedback queues and FCFS 
Processes in the lowest priority queues are scheduled in a FCFS order
        c. Priority and FCFS 
Equal priority processes are scheduled in a FCFS order.
        d. RR and SJF 
No relationship
    22. Describe the differences among short-term, medium-term, and long-term scheduling.

Short Answer
    • Short-term scheduling - also called "CPU scheduler", allocates CPU time to processes in memory 
    • Medium-term scheduling - swaps processes out of memory and reinstates them, to be run, at a later time, as in a time-sharing system 
    • Long-term scheduling - also called "job scheduler" 
A common difference, other than their defined tasks, is also how frequently each scheduler is called.
Long Answer
Short term scheduling concerns with the allocation of CPU time to processes in order to meet some pre-defined system performance objectives.
Medium term scheduling is essentially concerned with memory management, hence it's very often designed as a part of the memory management subsystem of an OS. Its efficient interaction with the short term scheduler is essential for system performances, especially in virtual memory systems. This is the reason why in paged system the pager process is usually run at a very high (dispatching) priority level. 
Long term scheduling obviously controls the degree of multiprogramming in multitasking systems, following certain policies to decide whether the system can honour a new job submission or, if more than one job is submitted, which of them should be selected. The need for some form of compromise between degree of multiprogramming and throughput seems evident, especially when one considers interactive systems.

    23. Describe with the aid of a diagram the life-cycle of a process. You should describe each of the states that it can be in, and the reasons why it moves between these states. 



As a process executes, it changes state
    • new:  The process is being created.
    • running:  Instructions are being executed.
    • waiting:  The process is waiting for some event to occur.
    • ready:  The process is waiting to be assigned to a processor.
    • terminated:  The process has finished execution.

Transition 1 occurs when process discovers that it cannot continue. If running process initiates an I/O operation before its allotted time expires, the running process voluntarily relinquishes the CPU. This state transition is: 
Block (process-name): Running → waiting.
Transition 2 occurs when the scheduler decides that the running process has run long enough and it is time to let another process have CPU time.
This state transition is: Time-Run-Out (process-name): Running → Ready. 
Transition 3 occurs when all other processes have had their share and it is time for the first process to run again
This state transition is: Dispatch (process-name): Ready → Running.
Transition 4 occurs when the external event for which a process was waiting (such as arrival of input) happens. This state transition is:                                          Wakeup (process-name): Waiting → Ready.
Transition 5 occurs when the process is created.
This state transition is: Admitted (process-name): New → Ready. 
Transition 6 occurs when the process has finished execution.
This state transition is: Exit (process-name): Running → Terminated.

    24. What information does the operating system keep in the process control block?

Information associated with each process.
    • Process state
    • Program counter
    • CPU registers
    • CPU scheduling information
    • Memory-management information
    • Accounting information
    • I/O status information

    25. Use an example to explain the need for process synchronization.

Cooperating processes may either share a logical address space (ie. Both code and data), or be allowed to share data through files. Process synchronization is, therefore, needed to prevent data inconsistencies where concurrent access by two processes on shared data may occur.

    26. What are critical section and race conditions?

    • Critical section: A section of code for which mutual exclusion must be implemented

    • Race condition: The situation where several processes access – and manipulate shared data concurrently. The final value of the shared data depends upon which process finishes last.

    27. Describe the purpose of semaphores and how they can be used to solve synchronization problems.

Semaphores are nonnegative integer variables that are used as a flag in and OS. The semaphore signals if and when a resource is free and can be used by a process

Semaphores are synchronization tools that can be used to solve synchronization problems by using the atomic operations: wait (p) and signal (v). For instance if two concurrent processes P1 and P2 with statements S1 and S2 respectively require that S2 be executed after S1 has completed then the scheme can be implemented by allowing P1 and P2 to share a common semaphore synch, initialized to 0, and by inserting statements in P1 to allow waiting and signaling and statements in P2 to wait, the synchronization problem can be solved.


    28. How can semaphores be used to protect critical sections? Which programming errors can occur?

A semaphore is assigned to the resource, it's shared among all processes that need to access the resource, and its counter is initialized to 1. A process then waits on the semaphore upon entering the critical section for the resource, and signals on leaving it. The first process will get access. If another process arrives while the former is still in the critical section, it'll be blocked, and so will further processes. In this situation the absolute value of the counter is equal to the number of processes waiting to enter the critical section. Every process leaving the critical section will let a waiting process use the resource by signaling the semaphore.

Programming errors
    • Starvation
    • Deadlock

    29. How can a message system be used to achieve synchronization?
Messages are yet another form of shared resource, through which cooperating processes can communicate data to each other. intuitively, we can think of implementing them by means of two primitives like 
    • send(dest, msg): to send a message to a designated process, destination of the message; 
    • receive(source, msg): to receive a message from a designated process; 
Synchronization: the primitives can cause the destination or source of the message to block until the delivery is completed (successfully or not). In this case they are called blocking, and nonblocking otherwise. The reception can also be either explicitly confirmed through the use of the analog of a return receipt or not. 
    30. Explain the term deadlock.

A set of blocked processes each holding a resource and waiting to acquire a resource held by another process in the set.

    31. What are resource allocation graphs?
A set of vertices V and a set of edges E.
    • V is partitioned into two types:
        ◦ P = {P1, P2, …,Pn}, the set consisting of all the processes in the system.
        ◦ R = {R1, R2, …,Rm}, the set consisting of all resource types in the system.
    • request edge – directed edge P1 ®Rj
    • assignment edge – directed edge Rj ®Pi

    32. Name the necessary conditions for a deadlock.

Deadlock can arise if four conditions hold simultaneously.

    • Mutual exclusion:  only one process at a time can use a resource.
    • Hold and wait:  a process holding at least one resource is waiting to acquire additional resources held by other processes.
    • No preemption:  a resource can be released only voluntarily by the process holding it, after that process has completed its task.
    • Circular wait:  there exists a set {P0, P1, …, P0} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for a resource that is held by P2, …, Pn–1 is waiting for a resource that is held by 
Pn, and P0 is waiting for a resource that is held by P0.

    33. Can deadlocks be prevented?

Yes, deadlocks can be prevented when you ensure that at least one of the four conditions cannot hold

    • Mutual Exclusion – not required for sharable resources; must hold for nonsharable resources.
    • Hold and Wait – must guarantee that whenever a process requests a resource, it does not hold any other resources.
        ◦ Require process to request and be allocated all its resources before it begins execution, or allow process to request resources only when the process has none.
        ◦ Low resource utilization; starvation possible.
    • No Preemption –
        ◦ If a process that is holding some resources requests another resource that cannot be immediately allocated to it, then all resources currently being held are released.
        ◦ Preempted resources are added to the list of resources for which the process is waiting.
        ◦ Process will be restarted only when it can regain its old resources, as well as the new ones that it is requesting.
    • Circular Wait – impose a total ordering of all resource types, and require that each process requests resources in an increasing order of enumeration.

    34. Is it possible to detect deadlocks? If yes, how can detected deadlocks be resolved?

Yes, deadlocks can be detected using a detection algorithm (Periodically invoke an algorithm that searches for the deadlock), if the resource allocation graphs contains no cycles, then there are no deadlocks, if a graph contains a cycle: i.e. if only one instance per resource type, then there is a deadlock, if several instances per resource type there is a possibility of a deadlock
	
Deadlocks can be resolved in the following ways
    • Abort all deadlocked processes
    • Abort one process at a time until the deadlock cycle is eliminated.

    3. Memory

    35. Describe the following memory allocation algorithms: 
        a. First fit: Allocate the first hole that is big enough. 
        b. Best fit: Allocate the smallest hole that is big enough; must search entire list, unless ordered by size. Produces the smallest leftover hole.
        c. Worst fit: Allocate the largest hole; must also search entire list. Produces the largest leftover hole.
    36. Explain the term “swapping”.

Swapping is a mechanism of removing a process temporarily out of memory to a backing store and then, and then brought back into memory for continued execution.

Example: Assume a multiprogramming environment with a round robin CPU scheduling algorithm. When a quantum expires, the memory manager will start to swap out the process that just finished, and swap in another process to the memory space that has just been freed.


    37. What is the difference between internal and external fragmentation?

    • Internal Fragmentation – allocated memory may be slightly larger than requested memory; this size difference is memory internal to a partition, but not being used
    • External Fragmentation – total memory space exists to satisfy a request, but it is not contiguous; storage is fragmented into a large number of small holes

    38. Explain the term “segmentation”.

    • Memory-management scheme that supports user view of memory. 

    39. What is compaction and in which context is it used?

Compaction is referred to as garbage collection, performed by the OS to reclaim fragmented sections of the memory space. 



Its should be used when;
    • When a certain percentage of memory becomes busy, say 80%.  
    • When there are processes waiting to get in. 
    • After a prescribed amount of time has elapsed

    40. Explain the term “paging”.

Paging is a memory management scheme that permits the physical address space of a process to be noncontiguous.???

    41. What are page faults? How are they handled by an operating system?

Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 



    42. Explain the term “virtual memory”.

A technique that allows programs to be executed even though they are not stored entirely in memory. It gives a user the illusion that a large amount of main memory is available when, in fact, it is not. 

    43. Name three loading and three replacement strategies and discuss their advantages and disadvantages.













    44. Why are segmentation and paging sometimes combined into one scheme? 

To remove the problems of external fragmentation and compaction, because the pages are of fixed length

    45. In the context of memory management, under which circumstances do external and internal fragmentations occur? How can each be handled? 

Internal Fragmentation occurs when fixed partitions suffer from inefficient memory use - any process, no matter how small, occupies an entire partition.

Internal fragmentation can be solved by combining free areas of memory whenever possible and combining it to form a block that will be deallocate.

External Fragmentation occurs when process are loaded and removed from memory, the free memory space is broken down into little pieces. 

External fragmentation can be handle by compaction; the OS reclaims fragmented sections of the memory space and compacts them to make one large block of memory that is large enough to accommodate some or all of the jobs waiting to get in. 


    46. Under what circumstances do page faults occur? Describe the actions taken by the operating system when a page fault occurs. 
Page faults are traps that are caused when a running process (not entirely loaded) encounters an instruction that references a page not in memory. 

They are handled by the OS as follows;
    • Find the location of the desired page on disk.
    • Find a free frame:
        ◦ If there is a free frame, use it.
        ◦ If there is no free frame, use a page replacement algorithm (like FIFO, LRU) to select a victim frame. 
    • Read the desired page into the free frame.
    • Restart the instruction that was interrupted by the trap. The process can now access the page as though it had always been in memory. 

    47. When virtual memory is implemented in a computing system, it carries certain costs and certain benefits. List those costs and the benefits. It is possible for the costs to exceed the benefits. Explain what measures you can take to ensure that this imbalance does not occur.

Benefits
    • Users can write extremely very large programs without being concerned with the limitations of main memory. 
    • Increases multiprogramming with a corresponding increase in CPU utilization and throughput, but with no increase in response time or turnaround time. 
    • Less I/O would be needed to load or swap each user program into memory, so each user program would run faster. 

    48. What is the cause of thrashing? How does the system detect thrashing? Once it detects thrashing, what can the system do to eliminate this problem? 

Thrashing is caused when a process is busy swapping pages in and out. The process spend more time paging than executing

Thrashing can be detected by;
    • Low CPU utilization

Thrashing can be limited by using a local replacement algorithm; if one process starts thrashing, it cannot steal frames from another process and cause the latter to thrash also.

    49. What is meant by the term demand paging in a virtual memory management system, and how is it implemented?

Demand paging is swapping a process into memory when it is needed

Implementation
    • Page is needed Þ reference to it
    • invalid reference Þ abort
    • not-in-memory Þ bring to memory
